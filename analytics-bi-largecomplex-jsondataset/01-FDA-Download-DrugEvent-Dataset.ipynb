{"cells":[{"cell_type":"markdown","source":["### Download Human Drug Adverse Event dataset from OpenFDA website \n","[OpenFDA](https://open.fda.gov/data/downloads/) website makes a variety of datasets available for download manually or programmatically. In this particular sample we have implemented Python code to download all 1400+ files for the Human Drug Adverse Event dataset. The files are downloaded as zipped files which are unzipped after download.\n","\n","On successful execution of this Notebook you will have two folders in the Lakehouse:\n","1. With Zipped files downloaded from OpenFDA website\n","2. With unzipped files which will be used as source for creating flattend JSON tables in a subsequent step\n","\n","The Notebook execution can take 2-3 hours so good idea to run using Data Factory pipeline feature available in Microsoft Fabric.\n","\n","**Note**: Its important to keep in mind that the size of raw unzipped JSON files will be 400+GB"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"c4612bb4-d084-4ad0-b6b1-382e6c25e0fc"},{"cell_type":"code","source":["import requests\n","\n","#retrieve metadata json file which has details for all datasets and files in those datasets\n","response  = requests.get(\"https://api.fda.gov/download.json\")\n","download_metadata_json = response.json() if response and response.status_code == 200 else None\n","\n","if download_metadata_json:\n","    print(\"metadata json available\")\n","else:\n","    print(\"error: \" + download_metada_json)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6f373ae1-3e0a-4d2f-ae7a-22599d891f88"},{"cell_type":"code","source":["#setup directory paths where zipped and unzipped files will reside\n","download_dir_name = \"fda_ds\"\n","download_dir_path =\"Files/\" + download_dir_name + \"/\"\n","\n","unzip_dir_name = download_dir_name + \"_unzipped\"\n","unzip_dir_path =\"Files/\" + unzip_dir_name + \"/\""],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"34f7db6e-0fe6-4e15-98f5-d149a8977a66"},{"cell_type":"code","source":["#create directories in the Lakehouse Files areas for zipped and unzipped files\n","print(download_dir_path)\n","mssparkutils.fs.mkdirs(download_dir_path)\n","\n","print(unzip_dir_path)\n","mssparkutils.fs.mkdirs(unzip_dir_path)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0456a207-c0bc-455b-84de-0a2f175bdd2e"},{"cell_type":"code","source":["from urllib.parse import urlparse\n","\n","counter = 0\n","\n","#parse the Metadata JSON file and loop through downloading each files to Lakehouse\n","#there are a total of 1400+ files which are referred to as partitions in the metadata json\n","#python code to download is pretty basic but can be optimized to use distributed processing of Spark in a subsequent iteration of the release of this sample \n","for p in download_metadata_json['results']['drug']['event']['partitions']:    \n","\n","    counter = counter + 1\n","    file_display_name = p['display_name']\n","    file_url = p['file']\n","\n","    path = urlparse(file_url).path\n","    file_name = path.split(\"/\")[-1]\n","    file_year_quarter = path.split(\"/\")[-2]\n","    print(f\"Downloading File# {counter}: {file_year_quarter}-{file_name}\")\n","    \n","    r = requests.get(file_url, allow_redirects=True)\n","\n","    download_path =\"/lakehouse/default/Files/fda_ds/\"\n","    with open(download_path + file_year_quarter + \"-\" + file_name, 'wb') as f:\n","        f.write(r.content)          "],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"88960466-5c5d-42e4-8c0f-7ed19ab84bb4"},{"cell_type":"code","source":["import zipfile\n","\n","#function to unzip file\n","def unzip_file(zip_filepath, output_file):\n","    with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n","        for f in zip_ref.infolist():\n","            data = zip_ref.read(f)\n","            with open(output_file, 'wb') as fh:\n","                fh.write(data)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"642d0d36-2560-4958-a5b6-78cf6a2622c2"},{"cell_type":"code","source":["#loop through list of zipped files uncompressing each of them in a different folder\n","data_files = mssparkutils.fs.ls(download_dir_path)\n","\n","counter = 0\n","for data_file in data_files:\n","    counter = counter + 1\n","    print(f\"File# {counter}: {data_file.name}\")\n","    unzip_file(f\"/lakehouse/default/Files/fda_ds/{data_file.name}\", \"/lakehouse/default/\" + unzip_dir_path + data_file.name.replace(\".zip\",\"\"))\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"57b0db03-35cc-4a66-a8fb-6908034e0a44"}],"metadata":{"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}