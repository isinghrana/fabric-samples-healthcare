{"cells":[{"cell_type":"markdown","source":["### 01 - Download CMS Medicare Part D data files (CSV format) to Lakehouse\n","\n","- [CMS Medicare Part D Prescribers - by Provider and Drug](https://data.cms.gov/provider-summary-by-type-of-service/medicare-part-d-prescribers/medicare-part-d-prescribers-by-provider-and-drug) dataset is available for download from CMS Website. \n","- This Notebook use [Public API Open Data Catalog](https://data.cms.gov/data.json) metadata json file published by CMS to identify and download dataset files to the Lakehouse\n","- Dataset contains one file for each year, Title field available for each in Metadata json is used tor identity the year value. Example - Title \"Medicare Part D Prescribers - by Provider and Drug : 2016-12-31\" indicates the file is for the year 2016"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e179bf6f-ce5b-4063-892d-7448ee4f6b72"},{"cell_type":"code","source":["%run Utils"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7fadc510-4f3d-456d-bf03-0a193f4b8002"},{"cell_type":"code","source":["config_dict = get_config_dict()\n","#config_dict[\"lakehouse_id\"] = notebookutils.lakehouse.get(config_dict[\"lakehouse_name\"])['id']"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"506210d8-3241-4dee-8c14-b1d0be89aa6a"},{"cell_type":"code","source":["#create the sub-directory in Files folder where the CSV files will be downloaded\n","lakehouse_dir = \"Files/cms_raw\"\n","lakehouse_dir_abfss_full_path = get_full_abfss_path(config_dict['workspace_id'], config_dict['lakehouse_id'], lakehouse_dir)\n","\n","notebookutils.fs.mkdirs(lakehouse_dir_abfss_full_path)\n","\n","print(f\"lakehouse_dir: {lakehouse_dir}\")\n","print(f\"lakehouse_dir_abfss_full_path: {lakehouse_dir_abfss_full_path}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"762a0349-842d-485b-8955-42a317e01c81"},{"cell_type":"code","source":["#local mount path is required for plain python to save files to Lakehouse file section\n","mount_point = \"/mnt/lakehouse/\" + config_dict[\"lakehouse_name\"] + \"/\" + lakehouse_dir\n","lakehouse_dir_local_mount_path = mount_path_return_local_path(lakehouse_dir_abfss_full_path, mount_point)\n","\n","print(f'mount point: {mount_point}')\n","print(f\"lakehouse_dir_local_path: {lakehouse_dir_local_mount_path}\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"d64b9512-8cfa-4a4a-82a6-2dcc404daed9"},{"cell_type":"code","source":["#full_path = \"abfss://c8f59358-83d0-4711-8b57-ebc3c414b0a1@onelake.dfs.fabric.microsoft.com/93c691f7-1409-44ef-ab45-fa3a3fb74289/Files/temp\"\n","#mount_path = \"/mnt/cms_lakehouse2/tmp\"\n","#notebookutils.fs.mount(full_path, mount_path)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"8f36a0e4-69e1-4cfa-8a86-91790742f407"},{"cell_type":"code","source":["# Define the file name and content\n","#local_path = notebookutils.fs.getMountPath(mount_path)\n","#print(local_path)\n","#file_name = local_path +  \"/test_file.txt\"\n","#content = \"This is a test file.\\nIt contains sample text written using Python.\"\n","\n","# Write the content to the file\n","#with open(file_name, \"w\") as file:\n","#    file.write(content)\n","\n","#print(f\"File '{file_name}' has been created with sample content.\")\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a6fe8b87-076b-4cda-a155-10bce960cf64"},{"cell_type":"code","source":["# Documentation provided at the following location  https://data.cms.gov/sites/default/files/2024-05/39b98adf-b5e0-4487-a19e-4dc5c1503d41/API%20Guide%20Formatted%201_5.pdf\n","# was used as basis for the following code which parses the Public API Open Data Catalog json file to identity the dataset files \n","\n","import requests\n","url = \"https://data.cms.gov/data.json\"\n","title= \"Medicare Part D Prescribers - by Provider and Drug\"\n","csv_distros =[]\n","response = requests.request(\"GET\", url)\n","\n","if response.ok:\n","    response = response.json()\n","    dataset = response['dataset']\n","    for set in dataset:\n","        if title == set['title']:\n","            for distro in set['distribution']:\n","                if 'mediaType' in distro.keys():\n","                    if distro['mediaType'] == \"text/csv\":\n","                        csv_distros.append(distro)        \n","else:\n","    error_message = f\"An error occrred in downloading the files from CMS Website: {response}\"\n","    print(error_message)\n","    notebookutils.notebook.exit(error_message, 1)\n","\n","#print(csv_distros)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2fc3ab1f-681a-469f-936b-08592e3d25f8"},{"cell_type":"code","source":["#create spark dataframe with rows for all files for the dataset\n","#downloadURL and title are the 2 fields of interest which are added as column in the dataframe\n","selected_dataset = [{\"downloadURL\": obj[\"downloadURL\"], \"title\": obj[\"title\"]} for obj in csv_distros]\n","df = spark.createDataFrame(selected_dataset)\n","display(df)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"0596f3f7-f570-490c-996c-2e474890779f"},{"cell_type":"code","source":["from pyspark.sql.functions import regexp_extract\n","\n","#identify Year value from the Title and add that as a column to dataframe\n","df = df.withColumn(\"year\", regexp_extract(\"title\", r\"(\\d{4})\", 1))\n","display(df)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"36c20ad6-1514-4170-9b50-a7ab3e944fda"},{"cell_type":"code","source":["#todo: test purpose limiting to download of just 1 file\n","df = df.limit(1)\n","display(df)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"dda3e4c7-1f31-4458-825c-a89388f7a6d1"},{"cell_type":"code","source":["import random, time\n","\n","\n","#function to download the file from URL\n","def download_file(url, filename, retries = 3, interval = 30):\n","\n","    attempt = 0\n","\n","    #usually APIs are rate limited so good idea to have retry pattern implemented for downloads\n","    while attempt < retries:\n","        try:\n","\n","            response = requests.get(url)    \n","            print(f\"Status Code: {response.status_code}\")  # Print the status code\n","            response.raise_for_status()  # Check if the request was successful\n","\n","            with open(filename, 'wb') as file:\n","                file.write(response.content)\n","            \n","            #file downloaded succesfully so break out of the while loop\n","            break       \n","        except requests.exceptions.RequestException as e:            \n","\n","            attempt += 1\n","            print(f\"Attempt {attempt} failed: {e}\")\n","\n","            if attempt < retries:\n","                print(f\"Retrying in {interval} seconds...\")\n","                interval = random.randint(15, 45)\n","                time.sleep(interval)\n","            else:\n","                print(\"All attempts failed. Download unsuccessful.\")\n","                raise Exception(\"Failed to download file after multiple attempts\")\n","\n","#function to process each DataFrame Row which corresponds to single file in teh dataset\n","#downloaded file is named based on the year value associated with data\n","def process_partition(partition):\n","    for row in partition:\n","        year_value = row['year']\n","        #output_file = \"/lakehouse/default/\" + lakehouse_dir + \"/\" + row['year'] + \".csv\"\n","        output_file = lakehouse_dir_local_mount_path + \"/\" + row['year'] + \".csv\"\n","        download_file(row['downloadURL'], output_file)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"155738ee-aa7a-4aeb-8fff-9bc140ebe7eb"},{"cell_type":"code","source":["#process the dataframe where each row represents a file to be downloaded from CMS file\n","df.rdd.foreachPartition(process_partition)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"77628dc7-17a0-4061-94d1-527d91423819"},{"cell_type":"code","source":["notebookutils.fs.unmount(mount_point)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a08e4686-2bf4-4f8c-8810-d4efaa346483"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}