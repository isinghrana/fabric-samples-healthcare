{"cells":[{"cell_type":"code","source":["%pip install semantic-link==0.11.1 semantic-link-labs==0.11.2"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c587f17c-8494-4426-8ff4-9fb053425666"},{"cell_type":"code","source":["#initialization of variables \n","\n","# these can be edited as per your need\n","# steps are skipped automatically if the corresponding item already exists\n","# lakehouse and notebook names cannot be reused if those items were recently deleted so that would be one reason to give a new name\n","lakehouse_name = \"cms_lakehouse\"\n","semantic_model_name = \"cms_semantic_model\" \n","datafactory_pipeline_name = \"cms_pipeline\"\n","report_name = \"cms_report\"\n","\n","download_cmsdata_notebook_import_name = \"01-DownloadCMSDataCsvFiles\"\n","create_data_table_notebook_import_name = \"02-CreateCMSDataTable\"\n","create_starschema_table_notebook_import_name = \"03-CreateCMSStarSchemaTables\"\n","\n","invoke_datafactory_pipeline_step = True"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"53e071e3-7dff-4542-981e-8eddbbb4e479"},{"cell_type":"code","source":["import requests\n","import zipfile\n","import os\n","import sempy_labs as labs\n","import sempy.fabric as semfabric\n","import base64\n","import json\n","\n","from urllib.parse import urlparse\n","\n","#initialization of additional variables which shouldn't be edited unless you know what you are doing\n","\n","#base directory is created in the lakehouse as part of the demo setup\n","base_dir_relative_path = \"Files/cmsdemofiles\"\n","\n","#external links\n","#artifact zip file from Github with definition files for Data Factory Pipeline, Power BI Semantic Model and Report\n","artifactzip_github_url = \"https://github.com/isinghrana/fabric-samples-healthcare/raw/refs/heads/isr-auto1/analytics-bi-directlake-starschema/demoautomation/artifacts.zip\"\n","\n","#github urls for notebook import\n","download_cmsdata_notebook_github_url = \"https://raw.githubusercontent.com/isinghrana/fabric-samples-healthcare/refs/heads/isr-auto1/analytics-bi-directlake-starschema/01-DownloadCMSDataCsvFiles.ipynb\"\n","create_data_table_notebook_github_url = \"https://raw.githubusercontent.com/isinghrana/fabric-samples-healthcare/refs/heads/isr-auto1/analytics-bi-directlake-starschema/02-CreateCMSDataTable.ipynb\"\n","create_starschema_table_notebook_github_url = \"https://raw.githubusercontent.com/isinghrana/fabric-samples-healthcare/refs/heads/isr-auto1/analytics-bi-directlake-starschema/03-CreateCMSStarSchemaTables.ipynb\"\n","\n","#data factory definition files are extracted from artifact zip file and paths are relative to the base_dir_relative_path\n","datafactory_pipeline_jsonfile_relativepath = \"/cms_pipeline.DataPipeline/pipeline-content.json\"\n","datafactory_platform_file_relativepath = \"/cms_pipeline.DataPipeline/.platform\"\n","\n","#these are fixed constant values from Data Factory Pipeline definition file\n","#DO NOT UPDATE unless the pipeline definition file is updated\n","datafactory_pipeline_downloadcmsdataset_notebookactivityname =  \"DownloadCMSDataset\"\n","datafactory_pipeline_createcmsdataflattable_notebookactivityname = \"CreateCMSDataFlatTable\"\n","datafactory_pipeline_createcmsstarschematables_notebookactivityname = \"CreateCMSStarSchemaTables\"\n","\n","#semantic model definition files are extracted from artifact zip file and paths are relative to the base dir\n","semanticmodel_relative_path = \"/CMS_Direct_Lake_Star_Schema.SemanticModel\"\n","report_relative_path = \"/CMS Medicare Part D Star Schema.Report\"\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4e1c2251-12b8-4150-8bf7-50f17ac77860"},{"cell_type":"code","source":["lakehouse_exists = any(item['displayName'] == lakehouse_name for item in notebookutils.lakehouse.list())\n","\n","if (lakehouse_exists):\n","    #lakehouse already exist so save the Id\n","    print(f'Lakehouse exists so using the existing lakehouse : {lakehouse_name}')\n","    lakehouse_id = notebookutils.lakehouse.get(lakehouse_name)['id']    \n","else:\n","    #create lakehouse as it does not exist\n","    print(f'Creating lakehouse : {lakehouse_name}')\n","    lakehouse = notebookutils.lakehouse.create(lakehouse_name)    \n","    lakehouse_id = lakehouse['id']\n","    \n","workspace_id = notebookutils.runtime.context[\"currentWorkspaceId\"]                                  \n","\n","#directory initialization\n","base_dir_full_path = f\"abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/{lakehouse_id}/{base_dir_relative_path}\"\n","notebookutils.fs.mkdirs(base_dir_full_path)\n","    \n","mount_point = \"/mnt/lakehouse/\" + lakehouse_name + \"/\" + base_dir_relative_path\n","print(f'base_dir full: {base_dir_full_path}, mount_point: {mount_point}')\n","\n","notebookutils.fs.mount(base_dir_full_path, mount_point)\n","base_dir_local_path = notebookutils.fs.getMountPath(mount_point)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"f526d945-1c5a-4154-9223-68dc19035f58"},{"cell_type":"code","source":["#common utility functions\n","\n","def get_file_contents(local_file_path):\n","    with open(local_file_path, \"r\", encoding=\"utf-8\") as file:\n","        file_content = file.read()\n","    return file_content\n","\n","#function is used in steps to import semantic model and report\n","#input arugment is folder with definition files\n","#directory and subdirectories are walked through and a dictionary returned where key is the part path and value is the content of the file\n","def get_fabricitemdef_partdict(definitionfiles_local_path) -> dict[str,str]:\n","\n","    def_dict = {}\n","\n","    for root, dirs, files in os.walk(definitionfiles_local_path):\n","        #print(f'Current directory: {root}')\n","        for file in files:\n","            #print(f'  File: {file}')\n","            part_key = root.replace(definitionfiles_local_path, \"\") + \"/\" + file\n","            part_key = part_key.lstrip('/')\n","            #print(f'part_key: {part_key}')\n","\n","            with open( root + \"/\" + file, \"r\", encoding=\"utf-8\") as file:\n","                payload = file.read()\n","                def_dict[part_key] = payload\n","\n","    return def_dict    \n","\n","def fabriclient_post(url, request_body):\n","\n","    client = semfabric.FabricRestClient()\n","    #print(create_datafactory_pipeline_request_body)\n","    response = client.request(method = \"POST\", path_or_url=url, lro_wait=True, json = request_body)\n","    print(response.status_code)\n","    print(response.text)\n","    response.raise_for_status()  # Raise an error for bad status codes   \n","\n","# used for data factory pipeline, semantic model and report\n","# each of these artifacts have .platform file with name of the artifact \n","# and this function updates the displayName attribute\n","def update_displayname_platformfile(json_str, display_name) -> str:\n","\n","    json_data = json.loads(json_str)\n","    json_data['metadata']['displayName'] = display_name\n","\n","    updated_json_str = json.dumps(json_data, indent=4)\n","    #print(updated_json_str)\n","    return updated_json_str\n","\n","\n","def item_exists(item_name, item_type) -> bool:\n","\n","    items_df = semfabric.list_items(item_type)\n","\n","    if item_name in items_df['Display Name'].values:\n","        print(f'{item_name} of type {item_type} exists')\n","        return True\n","    else:\n","        print(f'{item_name} of type {item_type} does not exist')\n","        return False\n","    "],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e777955d-a2a5-4f78-8eff-8eef61a8803d"},{"cell_type":"code","source":["#download artifacts zip file - Data Factory Pipeline, Semantic Model and REport files from GitHub which be used to create corresponding Fabric Items\n","\n","#function used to download artifact zip file\n","def download_binary_file(url, output_path):\n","    try:        \n","        response = requests.get(url=url, stream = True)\n","        \n","        response.raise_for_status()  # Raise an error for bad status codes\n","        with open(output_path, 'wb') as file:\n","            for chunk in response.iter_content(chunk_size=8192):\n","                file.write(chunk)\n","        print(f\"File downloaded successfully to: {output_path}\")\n","    except requests.exceptions.RequestException as e:\n","        print(f\"Download failed: {e}\")\n","        raise RuntimeError(f\"Failed to download file from {url}\") from e\n","\n","\n","def unzip_file(zip_path, extract_to):\n","    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","        zip_ref.extractall(extract_to)\n","    print(f\"Extracted all contents to '{extract_to}'\")\n","\n","artifact_filename = urlparse(artifactzip_github_url).path.split('/')[-1]\n","\n","if notebookutils.fs.exists(base_dir_full_path + \"/\" + artifact_filename):\n","    print (f\"{base_dir_full_path}/{artifact_filename} already exists so skipping artifact zip file download step\")    \n","else:        \n","    # Extract the path and get the last part    \n","    download_path = base_dir_local_path + '/' + artifact_filename\n","    print(f'downloading artifacts zip from - {artifactzip_github_url} to location {download_path}')\n","    download_binary_file(artifactzip_github_url, download_path)\n","    print('artifact file downloaded successfully and now unzipping the artifact file')\n","    unzip_file(download_path, base_dir_local_path)\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"389afb67-f799-497b-b550-b342000462d1"},{"cell_type":"code","source":["#import notebooks\n","\n","#if notebook was recently deleted, name is not be reusable right away so either wait for 5 to 10 minutes or set a different import name for the notebook in the settings above\n","def import_notebook(notebook_import_name, githuburl, workspace_id, lakehouse_name) -> str:\n","    #import notebook and return notebookid\n","    \n","    if item_exists(notebook_import_name, \"Notebook\"):\n","        print(f'{notebook_import_name} already exists so skipping import')\n","    else:\n","        print(f'{notebook_import_name} does not exist so importing from {githuburl}')\n","        result = labs.import_notebook_from_web(notebook_name = notebook_import_name, url = githuburl)        \n","        \n","        #update the default lakehouse        \n","        notebookutils.notebook.updateDefinition(name = notebook_import_name, workspaceId = workspace_id, defaultLakehouse = lakehouse_name, defaultLakehouseWorkspace= workspace_id)\n","    \n","    notebook_id = semfabric.resolve_item_id(item_name = notebook_import_name, type = \"Notebook\")\n","    print(f\"notebookname: {notebook_import_name}, notebook_id: {notebook_id}\")\n","    return notebook_id\n","\n","\n","#import notebooks and get Notebook Ids for all 3 notebooks to be used in subsequent steps\n","download_cmsdata_notebook_id = import_notebook(download_cmsdata_notebook_import_name, download_cmsdata_notebook_github_url, workspace_id, lakehouse_name)\n","create_data_table_notebook_id = import_notebook(create_data_table_notebook_import_name, create_data_table_notebook_github_url, workspace_id, lakehouse_name)\n","create_starschema_table_notebook_id = import_notebook(create_starschema_table_notebook_import_name, create_starschema_table_notebook_github_url, workspace_id, lakehouse_name)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"218b469a-66a6-4486-bcc9-0b3cce9f14ac"},{"cell_type":"code","source":["#import data factory pipeline\n","\n","def update_pipeline_json(json_str, workspace_id, pipelineactivity_notebook_mapping) -> str:\n","    \n","    data = json.loads(json_str)    \n","\n","    for notebook_activity_name, notebook_id in pipelineactivity_notebook_mapping.items():\n","        \n","        for activity in data['properties'][\"activities\"]:\n","            \n","            if activity.get(\"name\") == notebook_activity_name:\n","                print(f'Replacing {notebook_activity_name} with {notebook_id}')\n","                                            \n","                activity[\"typeProperties\"][\"workspaceId\"] = workspace_id\n","                activity[\"typeProperties\"][\"notebookId\"] = notebook_id\n","\n","    updated_json_str = json.dumps(data, indent=4)\n","    #print(updated_json_str)\n","    return updated_json_str\n","\n","\n","if item_exists(datafactory_pipeline_name, \"DataPipeline\"):\n","    print(f'{datafactory_pipeline_name} exists so skipping the step')\n","else:\n","    print(f'{datafactory_pipeline_name} does not exist so creating the pipeline')\n","\n","    datafactory_pipeline_jsonfile_local_path = base_dir_local_path + datafactory_pipeline_jsonfile_relativepath\n","    datafactory_platform_file_local_path = base_dir_local_path + datafactory_platform_file_relativepath\n","\n","    #read file contents\n","    platform_file_payload =  get_file_contents(datafactory_platform_file_local_path)\n","    pipeline_json_payload =  get_file_contents(datafactory_pipeline_jsonfile_local_path)\n","\n","    #pipeline defintion has Json nodes for Notebook Activities, need to update the JSON with appropriate NotebookId from this workspace\n","    notebookmapping_dict = {\n","        datafactory_pipeline_downloadcmsdataset_notebookactivityname : download_cmsdata_notebook_id,\n","        datafactory_pipeline_createcmsdataflattable_notebookactivityname : create_data_table_notebook_id,\n","        datafactory_pipeline_createcmsstarschematables_notebookactivityname: create_starschema_table_notebook_id    \n","    }\n","\n","    #workspace id and notebook ids need to be updated/replaced from the origin pipeline definition json\n","    pipeline_json_payload = update_pipeline_json(pipeline_json_payload, workspace_id, notebookmapping_dict)\n","\n","    platform_file_payload = update_displayname_platformfile(platform_file_payload, datafactory_pipeline_name)\n","\n","    #create post request body\n","    create_datafactory_pipeline_request_body = {\n","        \"displayName\": datafactory_pipeline_name,\n","        \"description\": \"cms_pipeline to ingest and process data\",\n","        \"definition\" : {\n","            \"parts\": [\n","                {\n","                    \"path\": \"pipeline-content.json\",\n","                    \"payload\": base64.b64encode(pipeline_json_payload.encode('utf-8')),\n","                    \"payloadType\": \"InlineBase64\"\n","                },\n","                {\n","                    \"path\": \".platform\",\n","                    \"payload\": base64.b64encode(platform_file_payload.encode('utf-8')),\n","                    \"payloadType\": \"InlineBase64\"\n","                }\n","            ]\n","        }\n","    }\n","\n","    create_pipeline_uri = f\"v1/workspaces/{workspace_id}/dataPipelines\"\n","    client = semfabric.FabricRestClient()\n","  \n","    #print(create_datafactory_pipeline_request_body)\n","    create_datafactory_pipeline_response = client.request(method = \"POST\", path_or_url=create_pipeline_uri, lro_wait=True, json = create_datafactory_pipeline_request_body)\n","    print(create_datafactory_pipeline_response.status_code)\n","    print(create_datafactory_pipeline_response.text)\n","    create_datafactory_pipeline_response.raise_for_status()  # Raise an error for bad status codes   "],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ad8f31b3-fcba-46b3-b092-ca1a70c16523"},{"cell_type":"code","source":["#import semantic model\n","\n","if item_exists(semantic_model_name, \"SemanticModel\"):\n","    print(f'Semantic Model {semantic_model_name} already exists so skipping the step')\n","else:    \n","\n","    create_semantic_model_uri = f\"v1/workspaces/{workspace_id}/semanticModels\"\n","\n","    #start with body which will get populated using the model defintion \n","    create_semantic_model_request_body = {\n","        \"displayName\": semantic_model_name,\n","        \"description\": \"cms semantic model created using API\",\n","        \"definition\" : {\n","            \"parts\": []\n","            }\n","        }\n","\n","    #read the semantic model definition folder into a dictionary to be used to be populate the request body for API Post call \n","    semanticmodel_local_path = base_dir_local_path + semanticmodel_relative_path\n","    print(f'semantic model definition files path: {semanticmodel_local_path}')\n","\n","    semantic_model_part_dict = get_fabricitemdef_partdict(semanticmodel_local_path)   \n","   \n","    #populate the request body using dictionary\n","    for key, value in semantic_model_part_dict.items():        \n","\n","        if \".platform\" in key:\n","            value = update_displayname_platformfile(value, semantic_model_name)\n","\n","        new_part = {\n","            \"path\": key,\n","            \"payload\" : base64.b64encode(value.encode('utf-8')),\n","            \"payloadType\": \"inlineBase64\"\n","        }\n","\n","        create_semantic_model_request_body[\"definition\"][\"parts\"].append(new_part)\n","   \n","    fabriclient_post(create_semantic_model_uri, create_semantic_model_request_body)   \n","\n","    print('Semantic Model created successfully and updating the semantic model to point to lakehouse in this workspace')\n","    \n","    #update the semantic model to point to lakehouse in this workspace\n","    labs.directlake.update_direct_lake_model_lakehouse_connection(\n","        dataset = semantic_model_name,\n","        lakehouse =  lakehouse_name\n","    )"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"28dfc0c0-677e-45a2-8883-bb67731af55f"},{"cell_type":"code","source":["#import report\n","\n","def update_reportdef_semanticmodelid(report_def_str, id) -> str:\n","\n","    report_def_json = json.loads(report_def_str)\n","\n","    # Replace the pbiModelDatabaseName value    \n","    report_def_json[\"datasetReference\"][\"byConnection\"][\"pbiModelDatabaseName\"] = id\n","    # Convert back to JSON string\n","    updated_json_str = json.dumps(report_def_json, indent=4)\n","    #print(updated_json_str)\n","    return updated_json_str\n","\n","if item_exists(report_name, \"Report\"):\n","    print(f'Report {report_name} alerady exists so skipping the step')\n","else:    \n","    \n","    #need to get semantic model id because report definition.pbir file needs to be updated with the semantic model craeted as part of the setup\n","    #in this workspace\n","    semantic_model_id = semfabric.resolve_item_id(semantic_model_name, type = \"SemanticModel\")\n","    create_report_uri = f\"v1/workspaces/{workspace_id}/reports\"\n","\n","    #start with body which will get populated using the model defintion \n","    create_report_request_body = {\n","        \"displayName\": report_name,\n","        \"description\": \"report created using API\",\n","        \"definition\" : {\n","            \"parts\": []\n","            }\n","        }\n","\n","    #read the semantic model definition folder into a dictionary to be used to be populate the request body for API Post call \n","    report_local_path = base_dir_local_path + report_relative_path\n","    print(f'report definition files path: {report_local_path}')\n","\n","    report_part_dict = get_fabricitemdef_partdict(report_local_path)   \n","    \n","    #populate the request body using dictionary\n","    for key, value in report_part_dict.items():              \n","\n","        if (\"definition.pbir\" in key):\n","            value = update_reportdef_semanticmodelid(value, semantic_model_id)        \n","            #print(f'Updated definition json: {value}')\n","        elif (\".platform\" in key):\n","            value = update_displayname_platformfile(value, report_name)\n","\n","        new_part = {\n","            \"path\": key,\n","            \"payload\" : base64.b64encode(value.encode('utf-8')),\n","            \"payloadType\": \"inlineBase64\"\n","        }           \n","    \n","        create_report_request_body[\"definition\"][\"parts\"].append(new_part)\n","                \n","    fabriclient_post(create_report_uri, create_report_request_body)\n","    print('report created successfully')\n","    #labs.report.report_rebind(report=report_name,dataset=semantic_model_name)  \n","   "],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9a192acb-445b-4d37-a686-ec755cbba2e6"},{"cell_type":"code","source":["#invoke Data Factory Pipeline to load data to Lakehouse\n","\n","if not invoke_datafactory_pipeline_step:\n","    print('Skipping invocation of Data Factory Pipeline setp')\n","else:\n","    datafactory_pipeline_id = semfabric.resolve_item_id(datafactory_pipeline_name, type = \"DataPipeline\")\n","    print(datafactory_pipeline_id)\n","\n","    url = f\"v1/workspaces/{workspace_id}/items/{datafactory_pipeline_id}/jobs/instances?jobType=Pipeline\"\n","\n","    client = semfabric.FabricRestClient()\n","    response = client.request(method = \"POST\", path_or_url=url)\n","    print(response.status_code)\n","    print(response.text)\n","    response.raise_for_status()  # Raise an error for bad status codes   \n","\n","    print(\"Data Factory Pipeline Job submitted successfully - monitor Pipeline Run from Monitoring Hub or open the pipeline then use Run > View Run History menu to actively monitor the pipeline. Once pipeline job complete data is available in Lakehouse for querying and reporting\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":true,"run_control":{"frozen":false}},"id":"d5883c28-002a-4c30-8054-885fb7b53e1e"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"widgets":{},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}