{"cells":[{"cell_type":"markdown","source":["#### ğŸš€ Quick Install: End-to-End Microsoft Fabric Sample\n","\n","**Dataset**: https://data.cms.gov/provider-summary-by-type-of-service/medicare-part-d-prescribers/medicare-part-d-prescribers-by-provider-and-drug  \n","**Source**: Centers for Medicare & Medicaid Services (CMS)\n","\n","This notebook sets up a complete demo and learning environment in **Microsoft Fabric**, provisioning the following components:\n","\n","- A **Lakehouse** with structured storage  \n","- Supporting **Notebooks** for data exploration  \n","- A **Data Factory Pipeline** for data ingestion and transformation to build out the Medallion Architecture  \n","- A **Semantic Model** for reporting  \n","- A **Power BI Report** for visualization  \n","\n","To get started, simply click **Run All**. The notebook will execute in about **5â€“10 minutes**.  \n","The final step triggers the **Data Factory pipeline asynchronously**, which loads the data into the Lakehouse.  \n","This pipeline run typically takes **20 to 45 minutes** to complete, and since it runs asynchronously, the notebook session can be safely stopped after submission.\n","\n","---\n","\n","### ğŸ§  Learn by Doing\n","\n","This notebook is not just a quick installâ€”itâ€™s also a great learning resource for:\n","\n","- **Automation in Microsoft Fabric**\n","- **Calling Fabric REST APIs**\n","\n","Each major step is clearly documented with markdown cells to explain its purpose, making it easy to follow and adapt for your own scenarios.\n","\n","---\n","\n","### ğŸ”§ Libraries Used\n","\n","This notebook leverages two powerful libraries designed for automation in Microsoft Fabric:\n","\n","- [**Semantic Link (SemPy)**](https://learn.microsoft.com/en-us/python/api/semantic-link/overview-semantic-link?view=semantic-link-python): A Python SDK for interacting with Fabric resources, including REST API calls, item management, and long-running operations.\n","- [**Semantic Link Labs**](https://github.com/microsoft/semantic-link-labs) with additional utilities and examples for scripting and automation in Fabric environments.\n","\n","These libraries simplify authentication, abstract complex API workflows, and are ideal for building scalable automation solutions.\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"d0255c10-2b6c-4e3b-af71-46956a66aefc"},{"cell_type":"code","source":["%pip install semantic-link==0.12.1 semantic-link-labs==0.9.10 fabric-data-agent-sdk==0.1.14a0"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[3,4,5,6,7],"state":"finished","livy_statement_state":"available","session_id":"6fb88cc4-f296-4bd8-85e8-471919a7abd1","normalized_state":"finished","queued_time":"2025-10-14T16:17:00.968454Z","session_start_time":"2025-10-14T16:17:00.9688189Z","execution_start_time":"2025-10-14T16:17:14.9666505Z","execution_finish_time":"2025-10-14T16:17:56.7310115Z","parent_msg_id":"84509366-9550-48fc-a790-2a6d9d3c22b1"},"text/plain":"StatementMeta(, 6fb88cc4-f296-4bd8-85e8-471919a7abd1, 7, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting semantic-link==0.12.1\r\n  Downloading semantic_link-0.12.1-py3-none-any.whl.metadata (11 kB)\r\nCollecting semantic-link-labs==0.9.10\r\n  Downloading semantic_link_labs-0.9.10-py3-none-any.whl.metadata (26 kB)\r\nCollecting fabric-data-agent-sdk==0.1.14a0\r\n  Downloading fabric_data_agent_sdk-0.1.14a0-py3-none-any.whl.metadata (5.7 kB)\r\nCollecting semantic-link-sempy==0.12.1 (from semantic-link==0.12.1)\r\n  Downloading semantic_link_sempy-0.12.1-py3-none-any.whl.metadata (11 kB)\r\nCollecting semantic-link-functions-geopandas==0.12.1 (from semantic-link==0.12.1)\r\n  Downloading semantic_link_functions_geopandas-0.12.1-py3-none-any.whl.metadata (1.9 kB)\r\nCollecting semantic-link-functions-holidays==0.12.1 (from semantic-link==0.12.1)\r\n  Downloading semantic_link_functions_holidays-0.12.1-py3-none-any.whl.metadata (1.8 kB)\r\nCollecting semantic-link-functions-meteostat==0.12.1 (from semantic-link==0.12.1)\r\n  Downloading semantic_link_functions_meteostat-0.12.1-py3-none-any.whl.metadata (2.0 kB)\r\nCollecting semantic-link-functions-phonenumbers==0.12.1 (from semantic-link==0.12.1)\r\n  Downloading semantic_link_functions_phonenumbers-0.12.1-py3-none-any.whl.metadata (1.8 kB)\r\nCollecting semantic-link-functions-validators==0.12.1 (from semantic-link==0.12.1)\r\n  Downloading semantic_link_functions_validators-0.12.1-py3-none-any.whl.metadata (1.8 kB)\r\nCollecting anytree (from semantic-link-labs==0.9.10)\r\n  Downloading anytree-2.13.0-py3-none-any.whl.metadata (8.0 kB)\r\nRequirement already satisfied: powerbiclient in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-labs==0.9.10) (3.1.1)\r\nCollecting polib (from semantic-link-labs==0.9.10)\r\n  Downloading polib-1.2.0-py2.py3-none-any.whl.metadata (15 kB)\r\nCollecting jsonpath_ng (from semantic-link-labs==0.9.10)\r\n  Downloading jsonpath_ng-1.7.0-py3-none-any.whl.metadata (18 kB)\r\nCollecting openai>=1.57.0 (from fabric-data-agent-sdk==0.1.14a0)\r\n  Downloading openai-2.3.0-py3-none-any.whl.metadata (29 kB)\r\nCollecting httpx==0.27.2 (from fabric-data-agent-sdk==0.1.14a0)\r\n  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\r\nCollecting azure-kusto-data>=4.5.0 (from fabric-data-agent-sdk==0.1.14a0)\r\n  Downloading azure_kusto_data-5.0.5-py2.py3-none-any.whl.metadata (4.2 kB)\r\nCollecting azure-identity==1.17.1 (from fabric-data-agent-sdk==0.1.14a0)\r\n  Downloading azure_identity-1.17.1-py3-none-any.whl.metadata (79 kB)\r\n\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/79.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.4/79.4 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hCollecting markdown2==2.5.3 (from fabric-data-agent-sdk==0.1.14a0)\r\n  Downloading markdown2-2.5.3-py3-none-any.whl.metadata (2.1 kB)\r\nRequirement already satisfied: azure-core>=1.23.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-identity==1.17.1->fabric-data-agent-sdk==0.1.14a0) (1.30.2)\r\nRequirement already satisfied: cryptography>=2.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-identity==1.17.1->fabric-data-agent-sdk==0.1.14a0) (42.0.2)\r\nRequirement already satisfied: msal>=1.24.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-identity==1.17.1->fabric-data-agent-sdk==0.1.14a0) (1.25.0)\r\nRequirement already satisfied: msal-extensions>=0.3.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-identity==1.17.1->fabric-data-agent-sdk==0.1.14a0) (1.0.0)\r\nRequirement already satisfied: typing-extensions>=4.0.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-identity==1.17.1->fabric-data-agent-sdk==0.1.14a0) (4.9.0)\r\nRequirement already satisfied: anyio in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from httpx==0.27.2->fabric-data-agent-sdk==0.1.14a0) (4.2.0)\r\nRequirement already satisfied: certifi in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from httpx==0.27.2->fabric-data-agent-sdk==0.1.14a0) (2024.2.2)\r\nCollecting httpcore==1.* (from httpx==0.27.2->fabric-data-agent-sdk==0.1.14a0)\r\n  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\r\nRequirement already satisfied: idna in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from httpx==0.27.2->fabric-data-agent-sdk==0.1.14a0) (3.4)\r\nRequirement already satisfied: sniffio in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from httpx==0.27.2->fabric-data-agent-sdk==0.1.14a0) (1.3.0)\r\nCollecting geopandas (from semantic-link-functions-geopandas==0.12.1->semantic-link==0.12.1)\r\n  Downloading geopandas-1.1.1-py3-none-any.whl.metadata (2.3 kB)\r\nCollecting folium (from semantic-link-functions-geopandas==0.12.1->semantic-link==0.12.1)\r\n  Downloading folium-0.20.0-py2.py3-none-any.whl.metadata (4.2 kB)\r\nCollecting mapclassify (from semantic-link-functions-geopandas==0.12.1->semantic-link==0.12.1)\r\n  Downloading mapclassify-2.10.0-py3-none-any.whl.metadata (3.1 kB)\r\nRequirement already satisfied: holidays in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-functions-holidays==0.12.1->semantic-link==0.12.1) (0.48)\r\nCollecting meteostat (from semantic-link-functions-meteostat==0.12.1->semantic-link==0.12.1)\r\n  Downloading meteostat-1.7.6-py3-none-any.whl.metadata (4.6 kB)\r\nCollecting phonenumbers (from semantic-link-functions-phonenumbers==0.12.1->semantic-link==0.12.1)\r\n  Downloading phonenumbers-9.0.16-py2.py3-none-any.whl.metadata (11 kB)\r\nCollecting validators (from semantic-link-functions-validators==0.12.1->semantic-link==0.12.1)\r\n  Downloading validators-0.35.0-py3-none-any.whl.metadata (3.9 kB)\r\nRequirement already satisfied: clr-loader>=0.2.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy==0.12.1->semantic-link==0.12.1) (0.2.5)\r\nCollecting fabric-analytics-sdk==0.0.1 (from fabric-analytics-sdk[online-notebook]==0.0.1->semantic-link-sempy==0.12.1->semantic-link==0.12.1)\r\n  Downloading fabric_analytics_sdk-0.0.1-py3-none-any.whl.metadata (14 kB)\r\nRequirement already satisfied: graphviz>=0.20.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy==0.12.1->semantic-link==0.12.1) (0.20.1)\r\nCollecting azure-keyvault-secrets>=4.7.0 (from semantic-link-sempy==0.12.1->semantic-link==0.12.1)\r\n  Downloading azure_keyvault_secrets-4.10.0-py3-none-any.whl.metadata (18 kB)\r\nRequirement already satisfied: pyarrow>=12.0.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy==0.12.1->semantic-link==0.12.1) (14.0.2)\r\nRequirement already satisfied: pythonnet>=3.0.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy==0.12.1->semantic-link==0.12.1) (3.0.1)\r\nRequirement already satisfied: scikit-learn>=1.2.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy==0.12.1->semantic-link==0.12.1) (1.2.2)\r\nRequirement already satisfied: setuptools>=68.2.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy==0.12.1->semantic-link==0.12.1) (68.2.2)\r\nRequirement already satisfied: tqdm>=4.65.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy==0.12.1->semantic-link==0.12.1) (4.65.0)\r\nRequirement already satisfied: rich>=13.3.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy==0.12.1->semantic-link==0.12.1) (13.3.5)\r\nRequirement already satisfied: regex>=2023.8.8 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy==0.12.1->semantic-link==0.12.1) (2023.10.3)\r\nRequirement already satisfied: pandas>=1.5.3 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy==0.12.1->semantic-link==0.12.1) (2.1.4)\r\nRequirement already satisfied: pyspark>=3.4.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy==0.12.1->semantic-link==0.12.1) (3.5.1.5.4.20240407)\r\nRequirement already satisfied: requests>=2.31.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy==0.12.1->semantic-link==0.12.1) (2.31.0)\r\nRequirement already satisfied: aiohttp>=3.8.6 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy==0.12.1->semantic-link==0.12.1) (3.9.3)\r\nRequirement already satisfied: IPython>=8.14.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy==0.12.1->semantic-link==0.12.1) (8.20.0)\r\nRequirement already satisfied: tenacity>=8.2.3 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy==0.12.1->semantic-link==0.12.1) (8.2.3)\r\nCollecting fabric-analytics-notebook-plugin (from fabric-analytics-sdk[online-notebook]==0.0.1->semantic-link-sempy==0.12.1->semantic-link==0.12.1)\r\n  Downloading fabric_analytics_notebook_plugin-0.0.1-py3-none-any.whl.metadata (13 kB)\r\n"]},{"output_type":"stream","name":"stdout","text":["Collecting h11>=0.16 (from httpcore==1.*->httpx==0.27.2->fabric-data-agent-sdk==0.1.14a0)\r\n  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\r\nRequirement already satisfied: python-dateutil>=2.8.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-kusto-data>=4.5.0->fabric-data-agent-sdk==0.1.14a0) (2.8.2)\r\nCollecting requests>=2.31.0 (from semantic-link-sempy==0.12.1->semantic-link==0.12.1)\r\n  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\r\nINFO: pip is looking at multiple versions of azure-kusto-data to determine which version is compatible with other requirements. This could take a while.\r\nCollecting azure-kusto-data>=4.5.0 (from fabric-data-agent-sdk==0.1.14a0)\r\n  Downloading azure_kusto_data-5.0.4-py2.py3-none-any.whl.metadata (4.2 kB)\r\n  Downloading azure_kusto_data-5.0.3-py2.py3-none-any.whl.metadata (4.2 kB)\r\n  Downloading azure_kusto_data-5.0.2-py2.py3-none-any.whl.metadata (4.2 kB)\r\n  Downloading azure_kusto_data-5.0.1-py2.py3-none-any.whl.metadata (4.2 kB)\r\n  Downloading azure_kusto_data-5.0.0-py2.py3-none-any.whl.metadata (4.2 kB)\r\nCollecting requests>=2.31.0 (from semantic-link-sempy==0.12.1->semantic-link==0.12.1)\r\n  Downloading requests-2.32.0-py3-none-any.whl.metadata (4.6 kB)\r\nCollecting ijson~=3.1 (from azure-kusto-data>=4.5.0->fabric-data-agent-sdk==0.1.14a0)\r\n  Downloading ijson-3.4.0.post0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (23 kB)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests>=2.31.0->semantic-link-sempy==0.12.1->semantic-link==0.12.1) (2.0.4)\r\nRequirement already satisfied: urllib3<3,>=1.21.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests>=2.31.0->semantic-link-sempy==0.12.1->semantic-link==0.12.1) (2.1.0)\r\nRequirement already satisfied: distro<2,>=1.7.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from openai>=1.57.0->fabric-data-agent-sdk==0.1.14a0) (1.8.0)\r\nCollecting jiter<1,>=0.10.0 (from openai>=1.57.0->fabric-data-agent-sdk==0.1.14a0)\r\n  Downloading jiter-0.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\r\nCollecting pydantic<3,>=1.9.0 (from openai>=1.57.0->fabric-data-agent-sdk==0.1.14a0)\r\n  Downloading pydantic-2.12.2-py3-none-any.whl.metadata (85 kB)\r\n\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/85.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.8/85.8 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hCollecting typing-extensions>=4.0.0 (from azure-identity==1.17.1->fabric-data-agent-sdk==0.1.14a0)\r\n  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\r\nRequirement already satisfied: ply in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from jsonpath_ng->semantic-link-labs==0.9.10) (3.11)\r\nRequirement already satisfied: ipywidgets>=7.0.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from powerbiclient->semantic-link-labs==0.9.10) (8.1.2)\r\nRequirement already satisfied: jupyter-ui-poll>=0.1.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from powerbiclient->semantic-link-labs==0.9.10) (1.0.0)\r\nRequirement already satisfied: aiosignal>=1.1.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from aiohttp>=3.8.6->semantic-link-sempy==0.12.1->semantic-link==0.12.1) (1.2.0)\r\nRequirement already satisfied: attrs>=17.3.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from aiohttp>=3.8.6->semantic-link-sempy==0.12.1->semantic-link==0.12.1) (23.1.0)\r\nRequirement already satisfied: frozenlist>=1.1.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from aiohttp>=3.8.6->semantic-link-sempy==0.12.1->semantic-link==0.12.1) (1.4.0)\r\nRequirement already satisfied: multidict<7.0,>=4.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from aiohttp>=3.8.6->semantic-link-sempy==0.12.1->semantic-link==0.12.1) (6.0.4)\r\nRequirement already satisfied: yarl<2.0,>=1.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from aiohttp>=3.8.6->semantic-link-sempy==0.12.1->semantic-link==0.12.1) (1.9.3)\r\nRequirement already satisfied: six>=1.11.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-core>=1.23.0->azure-identity==1.17.1->fabric-data-agent-sdk==0.1.14a0) (1.16.0)\r\nRequirement already satisfied: isodate>=0.6.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-keyvault-secrets>=4.7.0->semantic-link-sempy==0.12.1->semantic-link==0.12.1) (0.6.1)\r\nCollecting azure-core>=1.23.0 (from azure-identity==1.17.1->fabric-data-agent-sdk==0.1.14a0)\r\n  Downloading azure_core-1.35.1-py3-none-any.whl.metadata (46 kB)\r\n\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/46.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hRequirement already satisfied: cffi>=1.13 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from clr-loader>=0.2.5->semantic-link-sempy==0.12.1->semantic-link==0.12.1) (1.16.0)\r\nRequirement already satisfied: decorator in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from IPython>=8.14.0->semantic-link-sempy==0.12.1->semantic-link==0.12.1) (5.1.1)\r\nRequirement already satisfied: jedi>=0.16 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from IPython>=8.14.0->semantic-link-sempy==0.12.1->semantic-link==0.12.1) (0.18.1)\r\nRequirement already satisfied: matplotlib-inline in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from IPython>=8.14.0->semantic-link-sempy==0.12.1->semantic-link==0.12.1) (0.1.6)\r\nRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from IPython>=8.14.0->semantic-link-sempy==0.12.1->semantic-link==0.12.1) (3.0.43)\r\nRequirement already satisfied: pygments>=2.4.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from IPython>=8.14.0->semantic-link-sempy==0.12.1->semantic-link==0.12.1) (2.15.1)\r\nRequirement already satisfied: stack-data in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from IPython>=8.14.0->semantic-link-sempy==0.12.1->semantic-link==0.12.1) (0.2.0)\r\nRequirement already satisfied: traitlets>=5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from IPython>=8.14.0->semantic-link-sempy==0.12.1->semantic-link==0.12.1) (5.7.1)\r\nRequirement already satisfied: pexpect>4.3 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from IPython>=8.14.0->semantic-link-sempy==0.12.1->semantic-link==0.12.1) (4.8.0)\r\nRequirement already satisfied: comm>=0.1.3 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from ipywidgets>=7.0.0->powerbiclient->semantic-link-labs==0.9.10) (0.2.1)\r\nRequirement already satisfied: widgetsnbextension~=4.0.10 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from ipywidgets>=7.0.0->powerbiclient->semantic-link-labs==0.9.10) (4.0.10)\r\nRequirement already satisfied: jupyterlab-widgets~=3.0.10 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from ipywidgets>=7.0.0->powerbiclient->semantic-link-labs==0.9.10) (3.0.10)\r\nRequirement already satisfied: PyJWT<3,>=1.0.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.24.0->azure-identity==1.17.1->fabric-data-agent-sdk==0.1.14a0) (2.4.0)\r\nRequirement already satisfied: portalocker<3,>=1.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from msal-extensions>=0.3.0->azure-identity==1.17.1->fabric-data-agent-sdk==0.1.14a0) (2.3.0)\r\nRequirement already satisfied: numpy<2,>=1.23.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from pandas>=1.5.3->semantic-link-sempy==0.12.1->semantic-link==0.12.1) (1.26.4)\r\nRequirement already satisfied: pytz>=2020.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from pandas>=1.5.3->semantic-link-sempy==0.12.1->semantic-link==0.12.1) (2023.3.post1)\r\nRequirement already satisfied: tzdata>=2022.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from pandas>=1.5.3->semantic-link-sempy==0.12.1->semantic-link==0.12.1) (2023.3)\r\nCollecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai>=1.57.0->fabric-data-agent-sdk==0.1.14a0)\r\n  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\r\nCollecting pydantic-core==2.41.4 (from pydantic<3,>=1.9.0->openai>=1.57.0->fabric-data-agent-sdk==0.1.14a0)\r\n  Downloading pydantic_core-2.41.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\r\nCollecting typing-inspection>=0.4.2 (from pydantic<3,>=1.9.0->openai>=1.57.0->fabric-data-agent-sdk==0.1.14a0)\r\n  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\r\nRequirement already satisfied: py4j==0.10.9.7 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from pyspark>=3.4.1->semantic-link-sempy==0.12.1->semantic-link==0.12.1) (0.10.9.7)\r\nRequirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from rich>=13.3.5->semantic-link-sempy==0.12.1->semantic-link==0.12.1) (2.2.0)\r\nRequirement already satisfied: scipy>=1.3.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from scikit-learn>=1.2.2->semantic-link-sempy==0.12.1->semantic-link==0.12.1) (1.11.4)\r\nRequirement already satisfied: joblib>=1.1.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from scikit-learn>=1.2.2->semantic-link-sempy==0.12.1->semantic-link==0.12.1) (1.2.0)\r\nRequirement already satisfied: threadpoolctl>=2.0.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from scikit-learn>=1.2.2->semantic-link-sempy==0.12.1->semantic-link==0.12.1) (2.2.0)\r\nCollecting branca>=0.6.0 (from folium->semantic-link-functions-geopandas==0.12.1->semantic-link==0.12.1)\r\n  Downloading branca-0.8.2-py3-none-any.whl.metadata (1.7 kB)\r\nRequirement already satisfied: jinja2>=2.9 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from folium->semantic-link-functions-geopandas==0.12.1->semantic-link==0.12.1) (3.1.3)\r\nRequirement already satisfied: xyzservices in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from folium->semantic-link-functions-geopandas==0.12.1->semantic-link==0.12.1) (2022.9.0)\r\nCollecting pyogrio>=0.7.2 (from geopandas->semantic-link-functions-geopandas==0.12.1->semantic-link==0.12.1)\r\n  Downloading pyogrio-0.11.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (5.3 kB)\r\nRequirement already satisfied: packaging in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from geopandas->semantic-link-functions-geopandas==0.12.1->semantic-link==0.12.1) (23.1)\r\n"]},{"output_type":"stream","name":"stdout","text":["Collecting pyproj>=3.5.0 (from geopandas->semantic-link-functions-geopandas==0.12.1->semantic-link==0.12.1)\r\n  Downloading pyproj-3.7.2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (31 kB)\r\nCollecting shapely>=2.0.0 (from geopandas->semantic-link-functions-geopandas==0.12.1->semantic-link==0.12.1)\r\n  Downloading shapely-2.1.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.8 kB)\r\nCollecting networkx>=3.2 (from mapclassify->semantic-link-functions-geopandas==0.12.1->semantic-link==0.12.1)\r\n  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\r\nCollecting scikit-learn>=1.2.2 (from semantic-link-sempy==0.12.1->semantic-link==0.12.1)\r\n  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\r\nCollecting scipy>=1.3.2 (from scikit-learn>=1.2.2->semantic-link-sempy==0.12.1->semantic-link==0.12.1)\r\n  Downloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\r\n\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/62.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hCollecting threadpoolctl>=3.1.0 (from scikit-learn>=1.2.2->semantic-link-sempy==0.12.1->semantic-link==0.12.1)\r\n  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\r\nRequirement already satisfied: pycparser in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from cffi>=1.13->clr-loader>=0.2.5->semantic-link-sempy==0.12.1->semantic-link==0.12.1) (2.21)\r\nRequirement already satisfied: parso<0.9.0,>=0.8.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from jedi>=0.16->IPython>=8.14.0->semantic-link-sempy==0.12.1->semantic-link==0.12.1) (0.8.3)\r\nRequirement already satisfied: MarkupSafe>=2.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from jinja2>=2.9->folium->semantic-link-functions-geopandas==0.12.1->semantic-link==0.12.1) (2.1.3)\r\nRequirement already satisfied: mdurl~=0.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=13.3.5->semantic-link-sempy==0.12.1->semantic-link==0.12.1) (0.1.0)\r\nRequirement already satisfied: ptyprocess>=0.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from pexpect>4.3->IPython>=8.14.0->semantic-link-sempy==0.12.1->semantic-link==0.12.1) (0.7.0)\r\nRequirement already satisfied: wcwidth in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->IPython>=8.14.0->semantic-link-sempy==0.12.1->semantic-link==0.12.1) (0.2.5)\r\nRequirement already satisfied: psutil in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from fabric-analytics-notebook-plugin->fabric-analytics-sdk[online-notebook]==0.0.1->semantic-link-sempy==0.12.1->semantic-link==0.12.1) (5.9.0)\r\nRequirement already satisfied: executing in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from stack-data->IPython>=8.14.0->semantic-link-sempy==0.12.1->semantic-link==0.12.1) (0.8.3)\r\nRequirement already satisfied: asttokens in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from stack-data->IPython>=8.14.0->semantic-link-sempy==0.12.1->semantic-link==0.12.1) (2.0.5)\r\nRequirement already satisfied: pure-eval in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from stack-data->IPython>=8.14.0->semantic-link-sempy==0.12.1->semantic-link==0.12.1) (0.2.2)\r\n\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'requests' candidate (version 2.32.0 at https://files.pythonhosted.org/packages/24/e8/09e8d662a9675a4e4f5dd7a8e6127b463a091d2703ed931a64aa66d00065/requests-2.32.0-py3-none-any.whl (from https://pypi.org/simple/requests/) (requires-python:>=3.8))\r\nReason for being yanked: Yanked due to conflicts with CVE-2024-35195 mitigation\u001b[0m\u001b[33m\r\n\u001b[0mDownloading semantic_link-0.12.1-py3-none-any.whl (10 kB)\r\nDownloading semantic_link_labs-0.9.10-py3-none-any.whl (699 kB)\r\n\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/699.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m699.3/699.3 kB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading fabric_data_agent_sdk-0.1.14a0-py3-none-any.whl (48 kB)\r\n\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/48.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.7/48.7 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading azure_identity-1.17.1-py3-none-any.whl (173 kB)\r\n\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/173.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m173.2/173.2 kB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\r\n\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/76.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading markdown2-2.5.3-py3-none-any.whl (48 kB)\r\n\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/48.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading semantic_link_functions_geopandas-0.12.1-py3-none-any.whl (4.0 kB)\r\nDownloading semantic_link_functions_holidays-0.12.1-py3-none-any.whl (4.2 kB)\r\nDownloading semantic_link_functions_meteostat-0.12.1-py3-none-any.whl (4.5 kB)\r\nDownloading semantic_link_functions_phonenumbers-0.12.1-py3-none-any.whl (4.3 kB)\r\nDownloading semantic_link_functions_validators-0.12.1-py3-none-any.whl (4.8 kB)\r\nDownloading semantic_link_sempy-0.12.1-py3-none-any.whl (3.3 MB)\r\n\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/3.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m153.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading fabric_analytics_sdk-0.0.1-py3-none-any.whl (34 kB)\r\nDownloading httpcore-1.0.9-py3-none-any.whl (78 kB)\r\n\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/78.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading azure_kusto_data-5.0.0-py2.py3-none-any.whl (52 kB)\r\n\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/52.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading requests-2.32.0-py3-none-any.whl (63 kB)\r\n\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/63.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading openai-2.3.0-py3-none-any.whl (999 kB)\r\n\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/999.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m999.8/999.8 kB\u001b[0m \u001b[31m128.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading anytree-2.13.0-py3-none-any.whl (45 kB)\r\n\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/45.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading jsonpath_ng-1.7.0-py3-none-any.whl (30 kB)\r\nDownloading polib-1.2.0-py2.py3-none-any.whl (20 kB)\r\nDownloading azure_keyvault_secrets-4.10.0-py3-none-any.whl (125 kB)\r\n\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/125.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading azure_core-1.35.1-py3-none-any.whl (211 kB)\r\n\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/211.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.8/211.8 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading ijson-3.4.0.post0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (134 kB)\r\n\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/134.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.4/134.4 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading jiter-0.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)\r\n\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/349.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m349.0/349.0 kB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading pydantic-2.12.2-py3-none-any.whl (460 kB)\r\n\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/460.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m460.6/460.6 kB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading pydantic_core-2.41.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\r\n\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/2.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m120.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\r\n\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/44.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading folium-0.20.0-py2.py3-none-any.whl (113 kB)\r\n\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/113.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m113.4/113.4 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading geopandas-1.1.1-py3-none-any.whl (338 kB)\r\n\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/338.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m338.4/338.4 kB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading mapclassify-2.10.0-py3-none-any.whl (882 kB)\r\n\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/882.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m882.2/882.2 kB\u001b[0m \u001b[31m117.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\r\n\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/9.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.0/9.7 MB\u001b[0m \u001b[31m181.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m197.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m135.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading meteostat-1.7.6-py3-none-any.whl (33 kB)\r\nDownloading phonenumbers-9.0.16-py2.py3-none-any.whl (2.6 MB)\r\n\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/2.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m147.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading validators-0.35.0-py3-none-any.whl (44 kB)\r\n\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/44.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\r\nDownloading branca-0.8.2-py3-none-any.whl (26 kB)\r\nDownloading h11-0.16.0-py3-none-any.whl (37 kB)\r\nDownloading networkx-3.5-py3-none-any.whl (2.0 MB)\r\n\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m137.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading pyogrio-0.11.1-cp311-cp311-manylinux_2_28_x86_64.whl (27.7 MB)\r\n\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/27.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.0/27.7 MB\u001b[0m \u001b[31m179.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.0/27.7 MB\u001b[0m \u001b[31m178.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.3/27.7 MB\u001b[0m \u001b[31m198.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m \u001b[32m27.2/27.7 MB\u001b[0m \u001b[31m219.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m27.7/27.7 MB\u001b[0m \u001b[31m221.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m27.7/27.7 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading pyproj-3.7.2-cp311-cp311-manylinux_2_28_x86_64.whl (9.5 MB)\r\n\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/9.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.0/9.5 MB\u001b[0m \u001b[31m151.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.3/9.5 MB\u001b[0m \u001b[31m108.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m115.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\r\n\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/35.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.5/35.9 MB\u001b[0m \u001b[31m254.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.7/35.9 MB\u001b[0m \u001b[31m268.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m26.9/35.9 MB\u001b[0m \u001b[31m263.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m271.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m271.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m271.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading shapely-2.1.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.1 MB)\r\n\u001b[?25l"]},{"output_type":"stream","name":"stdout","text":["   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/3.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\r\nDownloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\r\nDownloading fabric_analytics_notebook_plugin-0.0.1-py3-none-any.whl (20 kB)\r\n"]},{"output_type":"stream","name":"stdout","text":["Installing collected packages: polib, phonenumbers, validators, typing-extensions, threadpoolctl, shapely, scipy, requests, pyproj, pyogrio, networkx, markdown2, jsonpath_ng, jiter, ijson, h11, anytree, annotated-types, typing-inspection, scikit-learn, pydantic-core, httpcore, branca, azure-core, pydantic, meteostat, mapclassify, httpx, geopandas, folium, azure-keyvault-secrets, openai, azure-identity, fabric-analytics-sdk, fabric-analytics-notebook-plugin, azure-kusto-data, semantic-link-sempy, semantic-link-labs, semantic-link-functions-validators, semantic-link-functions-phonenumbers, semantic-link-functions-meteostat, semantic-link-functions-holidays, semantic-link-functions-geopandas, semantic-link, fabric-data-agent-sdk\r\n"]},{"output_type":"stream","name":"stdout","text":["  Attempting uninstall: typing-extensions\r\n    Found existing installation: typing_extensions 4.9.0\r\n    Not uninstalling typing-extensions at /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages, outside environment /nfs4/pyenv-14667e78-08cd-419a-8571-e231b0576cb9\r\n    Can't uninstall 'typing_extensions'. No files were found to uninstall.\r\n  Attempting uninstall: threadpoolctl\r\n    Found existing installation: threadpoolctl 2.2.0\r\n    Not uninstalling threadpoolctl at /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages, outside environment /nfs4/pyenv-14667e78-08cd-419a-8571-e231b0576cb9\r\n    Can't uninstall 'threadpoolctl'. No files were found to uninstall.\r\n  Attempting uninstall: scipy\r\n"]},{"output_type":"stream","name":"stdout","text":["    Found existing installation: scipy 1.11.4\r\n    Not uninstalling scipy at /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages, outside environment /nfs4/pyenv-14667e78-08cd-419a-8571-e231b0576cb9\r\n    Can't uninstall 'scipy'. No files were found to uninstall.\r\n"]},{"output_type":"stream","name":"stdout","text":["  Attempting uninstall: requests\r\n    Found existing installation: requests 2.31.0\r\n    Not uninstalling requests at /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages, outside environment /nfs4/pyenv-14667e78-08cd-419a-8571-e231b0576cb9\r\n    Can't uninstall 'requests'. No files were found to uninstall.\r\n  Attempting uninstall: networkx\r\n    Found existing installation: networkx 3.1\r\n    Not uninstalling networkx at /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages, outside environment /nfs4/pyenv-14667e78-08cd-419a-8571-e231b0576cb9\r\n    Can't uninstall 'networkx'. No files were found to uninstall.\r\n"]},{"output_type":"stream","name":"stdout","text":["  Attempting uninstall: scikit-learn\r\n    Found existing installation: scikit-learn 1.2.2\r\n    Not uninstalling scikit-learn at /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages, outside environment /nfs4/pyenv-14667e78-08cd-419a-8571-e231b0576cb9\r\n    Can't uninstall 'scikit-learn'. No files were found to uninstall.\r\n"]},{"output_type":"stream","name":"stdout","text":["  Attempting uninstall: azure-core\r\n    Found existing installation: azure-core 1.30.2\r\n    Not uninstalling azure-core at /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages, outside environment /nfs4/pyenv-14667e78-08cd-419a-8571-e231b0576cb9\r\n    Can't uninstall 'azure-core'. No files were found to uninstall.\r\n  Attempting uninstall: azure-identity\r\n    Found existing installation: azure-identity 1.15.0\r\n    Not uninstalling azure-identity at /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages, outside environment /nfs4/pyenv-14667e78-08cd-419a-8571-e231b0576cb9\r\n    Can't uninstall 'azure-identity'. No files were found to uninstall.\r\n"]},{"output_type":"stream","name":"stdout","text":["  Attempting uninstall: semantic-link-sempy\r\n    Found existing installation: semantic-link-sempy 0.11.0\r\n    Not uninstalling semantic-link-sempy at /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages, outside environment /nfs4/pyenv-14667e78-08cd-419a-8571-e231b0576cb9\r\n    Can't uninstall 'semantic-link-sempy'. No files were found to uninstall.\r\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\nnni 3.0 requires filelock<3.12, but you have filelock 3.13.1 which is incompatible.\r\nfsspec-wrapper 0.1.15 requires PyJWT>=2.6.0, but you have pyjwt 2.4.0 which is incompatible.\r\ndatasets 2.19.1 requires fsspec[http]<=2024.3.1,>=2023.1.0, but you have fsspec 2024.6.1 which is incompatible.\u001b[0m\u001b[31m\r\n\u001b[0mSuccessfully installed annotated-types-0.7.0 anytree-2.13.0 azure-core-1.35.1 azure-identity-1.17.1 azure-keyvault-secrets-4.10.0 azure-kusto-data-5.0.0 branca-0.8.2 fabric-analytics-notebook-plugin-0.0.1 fabric-analytics-sdk-0.0.1 fabric-data-agent-sdk-0.1.14a0 folium-0.20.0 geopandas-1.1.1 h11-0.16.0 httpcore-1.0.9 httpx-0.27.2 ijson-3.4.0.post0 jiter-0.11.0 jsonpath_ng-1.7.0 mapclassify-2.10.0 markdown2-2.5.3 meteostat-1.7.6 networkx-3.5 openai-2.3.0 phonenumbers-9.0.16 polib-1.2.0 pydantic-2.12.2 pydantic-core-2.41.4 pyogrio-0.11.1 pyproj-3.7.2 requests-2.32.0 scikit-learn-1.7.2 scipy-1.16.2 semantic-link-0.12.1 semantic-link-functions-geopandas-0.12.1 semantic-link-functions-holidays-0.12.1 semantic-link-functions-meteostat-0.12.1 semantic-link-functions-phonenumbers-0.12.1 semantic-link-functions-validators-0.12.1 semantic-link-labs-0.9.10 semantic-link-sempy-0.12.1 shapely-2.1.2 threadpoolctl-3.6.0 typing-extensions-4.15.0 typing-inspection-0.4.2 validators-0.35.0\r\n\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\r\nNote: you may need to restart the kernel to use updated packages.\n"]},{"output_type":"stream","name":"stdout","text":["Warning: PySpark kernel has been restarted to use updated packages.\n\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c587f17c-8494-4426-8ff4-9fb053425666"},{"cell_type":"markdown","source":["## Configure Resource and Notebook Names\n","\n","This cell initializes key variables that define the names of resources and notebooks used throughout the demo setup. These variables can be customized to avoid naming conflicts or reuse issues, especially if resources were recently deleted.\n","\n","- Resource creation (lakehouse, semantic model, report, pipeline) steps are automatically skipped if the corresponding resource already exists.\n","- You may change the default names for resources to avoid conflicts with previously deleted resources that are still locked.\n","- The flag `invoke_datafactory_pipeline_step` is set to `True` by default, which means the Data Factory pipeline will be triggered at the end of the notebook to load data. You can set this to `False` to skip pipeline execution.\n","\n"," "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"612bef2a-9910-4f9b-85c9-685054e7a77d"},{"cell_type":"code","source":["# name of the resources to be created:\n","lakehouse_name = \"cms_lakehouse\"\n","semantic_model_name = \"cms_semantic_model\" \n","datafactory_pipeline_name = \"cms_pipeline\"\n","report_name = \"cms_report\"\n","data_agent_name = \"cms_data_agent\"\n","\n","# Name for the Notebooks to be imported and executed\n","download_cmsdata_notebook_import_name = \"01-DownloadCMSDataCsvFiles\"\n","create_data_table_notebook_import_name = \"02-CreateCMSDataTable\"\n","create_starschema_table_notebook_import_name = \"03-CreateCMSStarSchemaTables\"\n","\n","invoke_datafactory_pipeline_step = True\n","invoke_data_agent_create_step = True"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"6fb88cc4-f296-4bd8-85e8-471919a7abd1","normalized_state":"finished","queued_time":"2025-10-14T16:17:00.969291Z","session_start_time":null,"execution_start_time":"2025-10-14T16:18:03.5684057Z","execution_finish_time":"2025-10-14T16:18:04.0009719Z","parent_msg_id":"715d6eae-9ce6-4e2a-9870-6a8ab7d1a438"},"text/plain":"StatementMeta(, 6fb88cc4-f296-4bd8-85e8-471919a7abd1, 9, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"53e071e3-7dff-4542-981e-8eddbbb4e479"},{"cell_type":"markdown","source":["## Import Libraries and Set Advanced Configuration\n","\n","This cell imports required libraries and then initialized key configuration variables\n","\n","### **Initialize Advanced Configuration Variables**\n","These variables define paths and URLs used for downloading and deploying demo components. You typically **do not need to modify these** unless you're customizing the demo setup.\n","\n","#### Key Configuration:\n","- **Base Directory**: `Files/cmsdemofiles` â€” where demo files will be stored in the lakehouse\n","- **Artifact ZIP URL**: GitHub link to the ZIP file containing definitions for:\n","  - Data Factory Pipeline\n","  - Power BI Semantic Model\n","  - Report\n","- **Notebook Import URLs**: GitHub links to the notebooks used in the demo\n","- **Relative Paths for Deployment Files**:\n","  - Data Factory pipeline JSON and platform files\n","  - Semantic model and report folders\n","#### Fixed Constants:\n","These are activity names used in the Data Factory pipeline definition. **Do not change** unless the pipeline definition is updated:\n","- `DownloadCMSDataset`\n","- `CreateCMSDataFlatTable`\n","- `CreateCMSStarSchemaTables`\n","\n","> âš ï¸ **Note**: These configurations are critical for the automation steps that follow. Any changes should be made with caution and a clear understanding of the dependencies.\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4eb8f44f-2232-43fa-86c5-3a56dc3e99b4"},{"cell_type":"code","source":["import requests\n","import zipfile\n","import os\n","import sempy_labs as labs\n","import sempy.fabric as semfabric\n","import base64\n","import json\n","\n","from urllib.parse import urlparse\n","from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DecimalType, LongType, DateType\n","\n","from fabric.dataagent.client import (\n","    FabricDataAgentManagement,\n","    create_data_agent\n",")\n","\n","#initialization of additional configuration variables which shouldn't be changed unless you know what you are doing\n","\n","#base directory is created in the lakehouse as part of the demo setup\n","base_dir_relative_path = \"Files/cmsdemofiles\"\n","\n","#external links\n","#artifact zip file from Github with definition files for Data Factory Pipeline, Power BI Semantic Model and Report\n","#artifactzip_github_url = \"https://github.com/isinghrana/fabric-samples-healthcare/raw/refs/heads/main/analytics-bi-directlake-starschema/demoautomation/artifacts.zip\"\n","artifactzip_github_url = \"https://github.com/isinghrana/fabric-samples-healthcare/raw/refs/heads/isr-dataagentauto1/analytics-bi-directlake-starschema/demoautomation/artifacts.zip\"\n","\n","#github urls from where notebooks for the sample are import\n","#download_cmsdata_notebook_github_url =\"https://raw.githubusercontent.com/isinghrana/fabric-samples-healthcare/refs/heads/isr-dataagentauto1/analytics-bi-directlake-starschema/01-DownloadCMSDataCsvFiles.ipynb\"\n","download_cmsdata_notebook_github_url = \"https://raw.githubusercontent.com/isinghrana/fabric-samples-healthcare/refs/heads/main/analytics-bi-directlake-starschema/01-DownloadCMSDataCsvFiles.ipynb\"\n","create_data_table_notebook_github_url = \"https://raw.githubusercontent.com/isinghrana/fabric-samples-healthcare/refs/heads/main/analytics-bi-directlake-starschema/02-CreateCMSDataTable.ipynb\"\n","create_starschema_table_notebook_github_url = \"https://raw.githubusercontent.com/isinghrana/fabric-samples-healthcare/refs/heads/main/analytics-bi-directlake-starschema/03-CreateCMSStarSchemaTables.ipynb\"\n","\n","#data factory definition files are extracted from artifact zip file, relative paths to files which are modified during deployment\n","datafactory_pipeline_jsonfile_relativepath = \"/cms_pipeline.DataPipeline/pipeline-content.json\"\n","datafactory_platform_file_relativepath = \"/cms_pipeline.DataPipeline/.platform\"\n","\n","#these are fixed constant values from Data Factory Pipeline definition file\n","#DO NOT UPDATE unless the pipeline definition file is updated\n","datafactory_pipeline_downloadcmsdataset_notebookactivityname =  \"DownloadCMSDataset\"\n","datafactory_pipeline_createcmsdataflattable_notebookactivityname = \"CreateCMSDataFlatTable\"\n","datafactory_pipeline_createcmsstarschematables_notebookactivityname = \"CreateCMSStarSchemaTables\"\n","\n","#semantic model definition and report files are extracted from artifact zip file, relative paths to the respective folders\n","semanticmodel_relative_path = \"/CMS_Direct_Lake_Star_Schema.SemanticModel\"\n","report_relative_path = \"/CMS Medicare Part D Star Schema.Report\"\n","\n","#files used for data agent creation\n","data_agent_instructions_path = \"/agent_instructions.txt\"\n","data_agent_datasource_instructions_path = \"/AI_Skills_01_NotesForModel.txt\"\n","data_agent_datasource_query_examples_path = \"/AI_Skills_02_SQL_Examples.json\"\n","\n","dim_drug_table_name = \"cms_provider_dim_drug\"\n","dim_geography_table_name = \"cms_provider_dim_geography\"\n","dim_provider_table_name = \"cms_provider_dim_provider\"\n","dim_year_table_name = \"cms_provider_dim_year\"\n","fact_costs_table_name = \"cms_provider_drug_costs_star\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","session_id":"6fb88cc4-f296-4bd8-85e8-471919a7abd1","normalized_state":"finished","queued_time":"2025-10-14T16:17:00.9715054Z","session_start_time":null,"execution_start_time":"2025-10-14T16:18:04.0031803Z","execution_finish_time":"2025-10-14T16:18:10.3250731Z","parent_msg_id":"fa41ad77-2321-4139-af7d-5ba210798a0d"},"text/plain":"StatementMeta(, 6fb88cc4-f296-4bd8-85e8-471919a7abd1, 10, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4e1c2251-12b8-4150-8bf7-50f17ac77860"},{"cell_type":"markdown","source":["## Create and Mount Lakehouse for Demo Files\n","\n","This cell ensures that the required **Lakehouse** is available and sets up the directory structure for storing demo installation files.\n","\n","### Key Actions:\n","\n","1. **Lakehouse Initialization**:\n","   - Checks if a Lakehouse named `cms_lakehouse` already exists.\n","   - If it exists, retrieves its ID.\n","   - If not, creates a new Lakehouse with the specified name.\n","\n","2. **Directory Setup**:\n","   - Constructs the full path (`base_dir_full_path`) for the demo files directory inside the Lakehouse.\n","   - Creates the directory if it doesn't exist.\n","\n","3. **Mounting the Lakehouse Directory**:\n","   - Mounts the Lakehouse directory to a local path (`mount_point`) so it can be accessed using standard Python file operations.\n","   - Saves the local mount path in `base_dir_local_path` for use in subsequent steps.\n","\n","> ğŸ“ **Note**: This setup allows seamless access to files stored in the Lakehouse using both Spark and Python code. It also ensures that the demo environment is isolated and reproducible.\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"05d02b2e-6152-4c15-9bdb-e35df3f5024a"},{"cell_type":"code","source":["# this cell creates lakehouse if it doesn't exist, creates the base directory where installation files are downloaded from Github\n","# notebook does not use any default lakehouse so mounting of the lakehouse as well as saving the local path to mount point in base_dir_local_path variable\n","\n","lakehouse_exists = any(item['displayName'] == lakehouse_name for item in notebookutils.lakehouse.list())\n","\n","if (lakehouse_exists):\n","    #lakehouse already exist so get the lakehouse id for the existing lakehouse\n","    print(f'Lakehouse exists so using the existing lakehouse : {lakehouse_name}')\n","    lakehouse_id = notebookutils.lakehouse.get(lakehouse_name)['id']    \n","else:\n","    #create lakehouse as it does not exist\n","    print(f'Creating lakehouse : {lakehouse_name}')\n","    lakehouse = notebookutils.lakehouse.create(lakehouse_name)    \n","    lakehouse_id = lakehouse['id']\n","    \n","workspace_id = notebookutils.runtime.context[\"currentWorkspaceId\"]                                  \n","\n","#directory initialization\n","#base_dir_full_path is the fully qualified path for the lakehouse directory which can be used in Spark code\n","base_dir_full_path = f\"abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/{lakehouse_id}/{base_dir_relative_path}\"\n","notebookutils.fs.mkdirs(base_dir_full_path)\n","\n","lakehouse_table_dir_full_path = f\"abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/{lakehouse_id}/Tables\"\n","\n","#mounting of the lakehouse directory     \n","mount_point = \"/mnt/lakehouse/\" + lakehouse_name + \"/\" + base_dir_relative_path\n","print(f'base_dir full: {base_dir_full_path}, mount_point: {mount_point}')\n","\n","notebookutils.fs.mount(base_dir_full_path, mount_point)\n","\n","#local path of the mount point is also saved in variable to be used in subsequent steps in the notebook, plain python code (non-Spark) requires local path \n","base_dir_local_path = notebookutils.fs.getMountPath(mount_point)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"6fb88cc4-f296-4bd8-85e8-471919a7abd1","normalized_state":"finished","queued_time":"2025-10-14T16:17:00.9736813Z","session_start_time":null,"execution_start_time":"2025-10-14T16:18:10.3270977Z","execution_finish_time":"2025-10-14T16:18:14.0787209Z","parent_msg_id":"cffa7987-470e-4c06-9c8a-492fce19cf05"},"text/plain":"StatementMeta(, 6fb88cc4-f296-4bd8-85e8-471919a7abd1, 11, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Lakehouse exists so using the existing lakehouse : cms_lakehouse\n"]},{"output_type":"stream","name":"stdout","text":["base_dir full: abfss://c40ce2b4-2211-4459-9645-909afd3b7084@onelake.dfs.fabric.microsoft.com/ebb9147f-0489-4a34-8db2-2f9c8891426d/Files/cmsdemofiles, mount_point: /mnt/lakehouse/cms_lakehouse/Files/cmsdemofiles\n"]},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"918ef05b-023b-4f92-8a8a-6ae76fc385e6","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 918ef05b-023b-4f92-8a8a-6ae76fc385e6)"},"metadata":{}}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"f526d945-1c5a-4154-9223-68dc19035f58"},{"cell_type":"code","source":["#common utility functions\n","\n","#reads file contents and returns it to the caller\n","#plain python code so need to use the local path\n","def get_file_contents(local_file_path):\n","    with open(local_file_path, \"r\", encoding=\"utf-8\") as file:\n","        file_content = file.read()\n","    return file_content\n","\n","#function is used in steps to import semantic model and report\n","#input arugment is folder with definition files\n","#directory and subdirectories are walked through and a dictionary returned where key is the part path and value is the content of the file\n","def get_fabricitemdef_partdict(definitionfiles_local_path) -> dict[str,str]:\n","\n","    def_dict = {}\n","\n","    for root, dirs, files in os.walk(definitionfiles_local_path):\n","        #print(f'Current directory: {root}')\n","        for file in files:\n","            #print(f'  File: {file}')\n","            part_key = root.replace(definitionfiles_local_path, \"\") + \"/\" + file\n","            part_key = part_key.lstrip('/')\n","            #print(f'part_key: {part_key}')\n","\n","            with open( root + \"/\" + file, \"r\", encoding=\"utf-8\") as file:\n","                payload = file.read()\n","                def_dict[part_key] = payload\n","\n","    return def_dict    \n","\n","# use FabricRestClient from Sempy library to make POST requests to REST API calls\n","# FabricRestClient is a convenient method available in Sempy library - https://learn.microsoft.com/en-us/python/api/semantic-link-sempy/sempy.fabric.fabricrestclient?view=semantic-link-python\n","# Library has quite a few advantages which you can read more here - https://fabric.guru/using-fabricrestclient-to-make-fabric-rest-api-calls\n","# couple clear ones to call out:\n","# 1. Don't have to generate and pass the authentication token, sempy uses the environment details to authenticate the user automatically\n","# 2. Abstraction of the Long Running Operation where library automatically checks for long running operations to be complete - https://learn.microsoft.com/en-us/rest/api/fabric/articles/long-running-operation\n","def fabriclient_post(url, request_body):\n","\n","    client = semfabric.FabricRestClient(credential=None)\n","    \n","    response = client.request(method = \"POST\", path_or_url=url, lro_wait=True, json = request_body)\n","    print(response.status_code)\n","    print(response.text)\n","    response.raise_for_status()  # Raise an error for bad status codes   \n","\n","# used for data factory pipeline, semantic model and report\n","# each of these artifacts have .platform file with name of the artifact \n","# and this function updates the displayName attribute\n","def update_displayname_platformfile(json_str, display_name) -> str:\n","\n","    json_data = json.loads(json_str)\n","    json_data['metadata']['displayName'] = display_name\n","\n","    updated_json_str = json.dumps(json_data, indent=4)\n","    #print(updated_json_str)\n","    return updated_json_str\n","\n","\n","def item_exists(item_name, item_type) -> bool:\n","\n","    items_df = semfabric.list_items(item_type)\n","\n","    if item_name in items_df['Display Name'].values:\n","        print(f'{item_name} of type {item_type} exists')\n","        return True\n","    else:\n","        print(f'{item_name} of type {item_type} does not exist')\n","        return False    \n","\n","def lakehouse_table_exists(workspace_id: str, lakehouse_id: str, table_name: str) -> bool:\n","    \"\"\"\n","    Checks if a table exists in the specified Lakehouse.\n","\n","    Args:\n","        workspace_id (str): The ID of the workspace.\n","        lakehouse_id (str): The ID of the lakehouse.\n","        table_name (str): The name of the table to check.\n","\n","    Returns:\n","        bool: True if the table exists, False otherwise.\n","    \"\"\"\n","    tables_df = labs.lakehouse.get_lakehouse_tables(workspace=workspace_id, lakehouse=lakehouse_id)\n","    return table_name in tables_df[\"Table Name\"].values\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","session_id":"6fb88cc4-f296-4bd8-85e8-471919a7abd1","normalized_state":"finished","queued_time":"2025-10-14T16:17:00.975994Z","session_start_time":null,"execution_start_time":"2025-10-14T16:18:14.0808304Z","execution_finish_time":"2025-10-14T16:18:14.5197969Z","parent_msg_id":"f68087ae-e4d6-4404-81bc-cf905cc914f5"},"text/plain":"StatementMeta(, 6fb88cc4-f296-4bd8-85e8-471919a7abd1, 12, Finished, Available, Finished)"},"metadata":{}}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e777955d-a2a5-4f78-8eff-8eef61a8803d"},{"cell_type":"markdown","source":["## Download and Unzip Demo Artifacts\n","\n","This step downloads the demo artifact ZIP file from GitHub and extracts its contents. The artifact includes:\n","\n","- **Data Factory Pipeline** definition files\n","- **Semantic Model** files\n","- **Report** definition files"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"90374540-900a-420a-9007-c1d74786ade1"},{"cell_type":"code","source":["#download artifacts zip file - Data Factory Pipeline, Semantic Model and REport files from GitHub which be used to create corresponding Fabric Items\n","\n","#function used to download artifact zip file\n","def download_binary_file(url, output_path):\n","    try:        \n","        response = requests.get(url=url, stream = True)\n","        \n","        response.raise_for_status()  # Raise an error for bad status codes\n","        with open(output_path, 'wb') as file:\n","            for chunk in response.iter_content(chunk_size=8192):\n","                file.write(chunk)\n","        print(f\"File downloaded successfully to: {output_path}\")\n","    except requests.exceptions.RequestException as e:\n","        print(f\"Download failed: {e}\")\n","        raise RuntimeError(f\"Failed to download file from {url}\") from e\n","\n","\n","def unzip_file(zip_path, extract_to):\n","    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","        zip_ref.extractall(extract_to)\n","    print(f\"Extracted all contents to '{extract_to}'\")\n","\n","artifact_filename = urlparse(artifactzip_github_url).path.split('/')[-1]\n","\n","if notebookutils.fs.exists(base_dir_full_path + \"/\" + artifact_filename):\n","    print (f\"{base_dir_full_path}/{artifact_filename} already exists so skipping artifact zip file download step\")    \n","else:            \n","    download_path = base_dir_local_path + '/' + artifact_filename\n","    print(f'downloading artifacts zip from - {artifactzip_github_url} to location {download_path}')\n","    download_binary_file(artifactzip_github_url, download_path)\n","    print('artifact file downloaded successfully and now unzipping the artifact file')\n","    unzip_file(download_path, base_dir_local_path)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":13,"statement_ids":[13],"state":"finished","livy_statement_state":"available","session_id":"6fb88cc4-f296-4bd8-85e8-471919a7abd1","normalized_state":"finished","queued_time":"2025-10-14T16:17:00.9780451Z","session_start_time":null,"execution_start_time":"2025-10-14T16:18:14.5218968Z","execution_finish_time":"2025-10-14T16:18:14.8974659Z","parent_msg_id":"184adda2-16ea-430d-871b-1bbd5764d775"},"text/plain":"StatementMeta(, 6fb88cc4-f296-4bd8-85e8-471919a7abd1, 13, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["abfss://c40ce2b4-2211-4459-9645-909afd3b7084@onelake.dfs.fabric.microsoft.com/ebb9147f-0489-4a34-8db2-2f9c8891426d/Files/cmsdemofiles/artifacts.zip already exists so skipping artifact zip file download step\n"]}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"389afb67-f799-497b-b550-b342000462d1"},{"cell_type":"markdown","source":["## Import Required Notebooks from GitHub\n","\n","This cell automates the import of three notebooks from GitHub into the current Fabric workspace. These notebooks are essential for executing different stages of the demo setup.\n","\n","These notebook IDs are stored in variables and used later when orchestrating the Data Factory pipeline.\n","\n","> â³ **Note**: If a notebook was recently deleted, its name may not be immediately reusable. You can either wait a few minutes or change the import name in the configuration cell to avoid conflicts.\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"415bb525-173f-4d57-a016-0a02b6051e4b"},{"cell_type":"code","source":["def import_notebook(notebook_import_name, githuburl, workspace_id, lakehouse_name) -> str:\n","    #import notebook and return notebookid\n","    \n","    if item_exists(notebook_import_name, \"Notebook\"):\n","        print(f'{notebook_import_name} already exists so skipping import')\n","    else:\n","        print(f'{notebook_import_name} does not exist so importing from {githuburl}')\n","        result = labs.import_notebook_from_web(notebook_name = notebook_import_name, url = githuburl)        \n","        \n","        #update the default lakehouse        \n","        notebookutils.notebook.updateDefinition(name = notebook_import_name, workspaceId = workspace_id, defaultLakehouse = lakehouse_name, defaultLakehouseWorkspace= workspace_id)\n","    \n","    notebook_id = semfabric.resolve_item_id(item_name = notebook_import_name, type = \"Notebook\")\n","    print(f\"notebookname: {notebook_import_name}, notebook_id: {notebook_id}\")\n","    return notebook_id\n","\n","\n","#import notebooks and get Notebook Ids for all 3 notebooks to be used in subsequent steps\n","download_cmsdata_notebook_id = import_notebook(download_cmsdata_notebook_import_name, download_cmsdata_notebook_github_url, workspace_id, lakehouse_name)\n","create_data_table_notebook_id = import_notebook(create_data_table_notebook_import_name, create_data_table_notebook_github_url, workspace_id, lakehouse_name)\n","create_starschema_table_notebook_id = import_notebook(create_starschema_table_notebook_import_name, create_starschema_table_notebook_github_url, workspace_id, lakehouse_name)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":14,"statement_ids":[14],"state":"finished","livy_statement_state":"available","session_id":"6fb88cc4-f296-4bd8-85e8-471919a7abd1","normalized_state":"finished","queued_time":"2025-10-14T16:17:00.9801673Z","session_start_time":null,"execution_start_time":"2025-10-14T16:18:14.8996185Z","execution_finish_time":"2025-10-14T16:18:22.9298749Z","parent_msg_id":"e3f1c739-b68a-4e7f-bf8f-607d3557602d"},"text/plain":"StatementMeta(, 6fb88cc4-f296-4bd8-85e8-471919a7abd1, 14, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["01-DownloadCMSDataCsvFiles of type Notebook exists\n01-DownloadCMSDataCsvFiles already exists so skipping import\nnotebookname: 01-DownloadCMSDataCsvFiles, notebook_id: 10152b5f-775c-4353-9769-1910819de88c\n02-CreateCMSDataTable of type Notebook exists\n02-CreateCMSDataTable already exists so skipping import\nnotebookname: 02-CreateCMSDataTable, notebook_id: 9d83162c-bb4d-4600-ae71-9401aa2affd9\n03-CreateCMSStarSchemaTables of type Notebook exists\n03-CreateCMSStarSchemaTables already exists so skipping import\nnotebookname: 03-CreateCMSStarSchemaTables, notebook_id: dd3c44a7-5a26-40a4-81ba-ffbf8f956090\n"]}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"218b469a-66a6-4486-bcc9-0b3cce9f14ac"},{"cell_type":"markdown","source":["## Create Data Factory Pipeline via REST API\n","\n","This cell automates creation of a **Data Factory pipeline** using the Fabric REST API if it doesn't already exist. The pipeline orchestrates the execution of imported notebooks to ingest and transform CMS data.\n","\n","**Key Steps**\n","- Reads the pipeline definition (`pipeline-content.json`) and metadata (`.platform`) from the extracted artifact files.\n","- Updates the pipeline JSON with workspace and notebook IDs using `update_pipeline_json` function. \n","- Updates the `.platform` file to set the correct display name.\n","- Constructs a POST request body with the updated pipeline definition and metadata.\n","- Uses `FabricRestClient` to send the request to the Fabric API endpoint."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b4e7545c-2474-42f0-82f7-78875ba6eb34"},{"cell_type":"code","source":["#import data factory pipeline using REST API - https://learn.microsoft.com/en-us/rest/api/fabric/datapipeline/items/create-data-pipeline?tabs=HTTP\n","\n","# Ensures each activity (e.g., `DownloadCMSDataset`, `CreateCMSDataFlatTable`, `CreateCMSStarSchemaTables`) is correctly linked to its corresponding notebook\n","# by updating the pipeline definition JSON by replacing placeholder notebook activity references with actual notebook IDs from the current workspace.\n","def update_pipeline_json(json_str, workspace_id, pipelineactivity_notebook_mapping) -> str:\n","    \n","    data = json.loads(json_str)    \n","\n","    for notebook_activity_name, notebook_id in pipelineactivity_notebook_mapping.items():\n","        \n","        for activity in data['properties'][\"activities\"]:\n","            \n","            if activity.get(\"name\") == notebook_activity_name:\n","                print(f'Replacing {notebook_activity_name} with {notebook_id}')\n","                                            \n","                activity[\"typeProperties\"][\"workspaceId\"] = workspace_id\n","                activity[\"typeProperties\"][\"notebookId\"] = notebook_id\n","\n","    updated_json_str = json.dumps(data, indent=4)\n","    #print(updated_json_str)\n","    return updated_json_str\n","\n","\n","if item_exists(datafactory_pipeline_name, \"DataPipeline\"):\n","    print(f'{datafactory_pipeline_name} exists so skipping the step')\n","else:\n","    print(f'{datafactory_pipeline_name} does not exist so creating the pipeline')\n","\n","    datafactory_pipeline_jsonfile_local_path = base_dir_local_path + datafactory_pipeline_jsonfile_relativepath\n","    datafactory_platform_file_local_path = base_dir_local_path + datafactory_platform_file_relativepath\n","\n","    #read file contents\n","    platform_file_payload =  get_file_contents(datafactory_platform_file_local_path)\n","    pipeline_json_payload =  get_file_contents(datafactory_pipeline_jsonfile_local_path)\n","\n","    #pipeline defintion has Json nodes for Notebook Activities, need to update the JSON with appropriate NotebookId from this workspace\n","    notebookmapping_dict = {\n","        datafactory_pipeline_downloadcmsdataset_notebookactivityname : download_cmsdata_notebook_id,\n","        datafactory_pipeline_createcmsdataflattable_notebookactivityname : create_data_table_notebook_id,\n","        datafactory_pipeline_createcmsstarschematables_notebookactivityname: create_starschema_table_notebook_id    \n","    }\n","\n","    #workspace id and notebook ids need to be updated/replaced from the original pipeline definition json\n","    pipeline_json_payload = update_pipeline_json(pipeline_json_payload, workspace_id, notebookmapping_dict)\n","\n","    platform_file_payload = update_displayname_platformfile(platform_file_payload, datafactory_pipeline_name)\n","\n","    #create post request body\n","    create_datafactory_pipeline_request_body = {\n","        \"displayName\": datafactory_pipeline_name,\n","        \"description\": \"cms_pipeline to ingest and process data\",\n","        \"definition\" : {\n","            \"parts\": [\n","                {\n","                    \"path\": \"pipeline-content.json\",\n","                    \"payload\": base64.b64encode(pipeline_json_payload.encode('utf-8')),\n","                    \"payloadType\": \"InlineBase64\"\n","                },\n","                {\n","                    \"path\": \".platform\",\n","                    \"payload\": base64.b64encode(platform_file_payload.encode('utf-8')),\n","                    \"payloadType\": \"InlineBase64\"\n","                }\n","            ]\n","        }\n","    }\n","\n","    create_pipeline_uri = f\"v1/workspaces/{workspace_id}/dataPipelines\"\n","    client = semfabric.FabricRestClient()\n","  \n","    #print(create_datafactory_pipeline_request_body)\n","    create_datafactory_pipeline_response = client.request(method = \"POST\", path_or_url=create_pipeline_uri, lro_wait=True, json = create_datafactory_pipeline_request_body)\n","    print(create_datafactory_pipeline_response.status_code)\n","    print(create_datafactory_pipeline_response.text)\n","    create_datafactory_pipeline_response.raise_for_status()  # Raise an error for bad status codes   "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":15,"statement_ids":[15],"state":"finished","livy_statement_state":"available","session_id":"6fb88cc4-f296-4bd8-85e8-471919a7abd1","normalized_state":"finished","queued_time":"2025-10-14T16:17:00.9826162Z","session_start_time":null,"execution_start_time":"2025-10-14T16:18:22.9321266Z","execution_finish_time":"2025-10-14T16:18:23.3106001Z","parent_msg_id":"0c51a398-4665-4878-a9b6-cee2eff95ec7"},"text/plain":"StatementMeta(, 6fb88cc4-f296-4bd8-85e8-471919a7abd1, 15, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["cms_pipeline of type DataPipeline exists\ncms_pipeline exists so skipping the step\n"]}],"execution_count":8,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ad8f31b3-fcba-46b3-b092-ca1a70c16523"},{"cell_type":"markdown","source":["## Import Semantic Model via REST API\n","\n","This step creates a **Power BI Semantic Model** in the Fabric workspace using the REST API if it doesn't already exist. The semantic model defines the structure and relationships of the CMS data for reporting and analysis.\n","\n","\n","**Key Steps**\n","- Reads all definition files from the semantic model folder (`CMS_Direct_Lake_Star_Schema.SemanticModel`) into a dictionary using `get_fabricitemdef_partdict`.\n","- Updates the .platform file to set the correct display name.\n","- Constructs a POST request body with the updated pipeline definition and metadata.\n","- Uses `FabricRestClient` to send the request to the Fabric API endpoint.\n","- Lastly, updates the Semantic Model to point to the Lakehouse from this Workspace"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8843f011-c355-4fd0-bddb-13b414b3e0a3"},{"cell_type":"code","source":["#import semantic model using REST API - https://learn.microsoft.com/en-us/rest/api/fabric/semanticmodel/items/create-semantic-model?tabs=HTTP\n","\n","if item_exists(semantic_model_name, \"SemanticModel\"):\n","    print(f'Semantic Model {semantic_model_name} already exists so skipping the step')\n","else:    \n","\n","    create_semantic_model_uri = f\"v1/workspaces/{workspace_id}/semanticModels\"\n","\n","    #start with body which will get populated using the model defintion \n","    create_semantic_model_request_body = {\n","        \"displayName\": semantic_model_name,\n","        \"description\": \"cms semantic model created using API\",\n","        \"definition\" : {\n","            \"parts\": []\n","            }\n","        }\n","\n","    #read the semantic model definition folder into a dictionary to be used to be populate the request body for API Post call \n","    semanticmodel_local_path = base_dir_local_path + semanticmodel_relative_path\n","    print(f'semantic model definition files path: {semanticmodel_local_path}')\n","\n","    semantic_model_part_dict = get_fabricitemdef_partdict(semanticmodel_local_path)   \n","   \n","    #populate the request body using dictionary\n","    for key, value in semantic_model_part_dict.items():        \n","\n","        if \".platform\" in key:\n","            value = update_displayname_platformfile(value, semantic_model_name)\n","\n","        new_part = {\n","            \"path\": key,\n","            \"payload\" : base64.b64encode(value.encode('utf-8')),\n","            \"payloadType\": \"inlineBase64\"\n","        }\n","\n","        create_semantic_model_request_body[\"definition\"][\"parts\"].append(new_part)\n","   \n","    fabriclient_post(create_semantic_model_uri, create_semantic_model_request_body)   \n","\n","    print('Semantic Model created successfully and updating the semantic model to point to lakehouse in this workspace')\n","    \n","    #update the semantic model to point to lakehouse in this workspace\n","    labs.directlake.update_direct_lake_model_lakehouse_connection(\n","        dataset = semantic_model_name,\n","        lakehouse =  lakehouse_name\n","    )"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":16,"statement_ids":[16],"state":"finished","livy_statement_state":"available","session_id":"6fb88cc4-f296-4bd8-85e8-471919a7abd1","normalized_state":"finished","queued_time":"2025-10-14T16:17:00.9849512Z","session_start_time":null,"execution_start_time":"2025-10-14T16:18:23.3128754Z","execution_finish_time":"2025-10-14T16:18:23.7089508Z","parent_msg_id":"2054da69-8cf6-4541-b8bb-9a82aa1e43df"},"text/plain":"StatementMeta(, 6fb88cc4-f296-4bd8-85e8-471919a7abd1, 16, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["cms_semantic_model of type SemanticModel exists\nSemantic Model cms_semantic_model already exists so skipping the step\n"]}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"28dfc0c0-677e-45a2-8883-bb67731af55f"},{"cell_type":"markdown","source":["## Import Power BI Report via REST API\n","\n","This step creates a **Power BI Report** in the Fabric workspace using the REST API if it doesn't alrady exist. The report is based on the semantic model created in the previous step and provides visual insights into CMS data.\n","\n","**Key Steps**\n","- Reads all definition files from the semantic model folder (`CMS_Direct_Lake_Star_Schema.Report`) into a dictionary using `report_part_dict`.\n","- Updates the `.platform` file to set the correct display name.\n","- Updates the `definition.pbir` file to bind the report to the semantic model.\n","- Constructs a POST request body with the updated pipeline definition and metadata.\n","- Uses `FabricRestClient` to send the request to the Fabric API endpoint."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9c75a01d-1a98-4f9b-8935-c4cb074e2d29"},{"cell_type":"code","source":["#import report using REST API - https://learn.microsoft.com/en-us/rest/api/fabric/report/items/create-report?tabs=HTTP\n","\n","def update_reportdef_semanticmodelid(report_def_str, id) -> str:\n","\n","    report_def_json = json.loads(report_def_str)\n","\n","    # Replace the pbiModelDatabaseName value    \n","    report_def_json[\"datasetReference\"][\"byConnection\"][\"pbiModelDatabaseName\"] = id\n","    # Convert back to JSON string\n","    updated_json_str = json.dumps(report_def_json, indent=4)\n","    #print(updated_json_str)\n","    return updated_json_str\n","\n","if item_exists(report_name, \"Report\"):\n","    print(f'Report {report_name} alerady exists so skipping the step')\n","else:    \n","    \n","    #need to get semantic model id because report definition.pbir file needs to be updated with the semantic model craeted as part of the setup\n","    #in this workspace\n","    semantic_model_id = semfabric.resolve_item_id(semantic_model_name, type = \"SemanticModel\")\n","    create_report_uri = f\"v1/workspaces/{workspace_id}/reports\"\n","\n","    #start with body which will get populated using the model defintion \n","    create_report_request_body = {\n","        \"displayName\": report_name,\n","        \"description\": \"report created using API\",\n","        \"definition\" : {\n","            \"parts\": []\n","            }\n","        }\n","\n","    #read the semantic model definition folder into a dictionary to be used to be populate the request body for API Post call \n","    report_local_path = base_dir_local_path + report_relative_path\n","    print(f'report definition files path: {report_local_path}')\n","\n","    report_part_dict = get_fabricitemdef_partdict(report_local_path)   \n","    \n","    #populate the request body using dictionary\n","    for key, value in report_part_dict.items():              \n","\n","        if (\"definition.pbir\" in key):\n","            value = update_reportdef_semanticmodelid(value, semantic_model_id)        \n","            #print(f'Updated definition json: {value}')\n","        elif (\".platform\" in key):\n","            value = update_displayname_platformfile(value, report_name)\n","\n","        new_part = {\n","            \"path\": key,\n","            \"payload\" : base64.b64encode(value.encode('utf-8')),\n","            \"payloadType\": \"inlineBase64\"\n","        }           \n","    \n","        create_report_request_body[\"definition\"][\"parts\"].append(new_part)\n","                \n","    fabriclient_post(create_report_uri, create_report_request_body)\n","    print('report created successfully')  "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":17,"statement_ids":[17],"state":"finished","livy_statement_state":"available","session_id":"6fb88cc4-f296-4bd8-85e8-471919a7abd1","normalized_state":"finished","queued_time":"2025-10-14T16:17:01.0852701Z","session_start_time":null,"execution_start_time":"2025-10-14T16:18:23.7111753Z","execution_finish_time":"2025-10-14T16:18:24.0912659Z","parent_msg_id":"21582e4b-442f-498d-a577-75655f9f3bc4"},"text/plain":"StatementMeta(, 6fb88cc4-f296-4bd8-85e8-471919a7abd1, 17, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["cms_report of type Report exists\nReport cms_report alerady exists so skipping the step\n"]}],"execution_count":10,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9a192acb-445b-4d37-a686-ec755cbba2e6"},{"cell_type":"markdown","source":["## Pre-Requisites for Fabric Data Agent\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"15d52c12-259c-4346-b7e7-f7d3bda1d3cc"},{"cell_type":"code","source":["if not invoke_data_agent_create_step:\n","    print(\"Skipping Data Agent Pre-Requisite Step\")\n","else:\n","\n","    #####################\n","    #Step 1: Create empty Lakehouse Tables with appropriate schema as they need to be specified as data source for the Data Agent. Data will\n","    #         be loaded into tables by Pipeline\n","\n","    # Create empty Lakehouse Tables only if they don't already exis\n","\n","    if not lakehouse_table_exists(workspace_id, lakehouse_id, dim_drug_table_name):\n","        print(f'Creating {dim_drug_table_name}...')\n","        # Define schema\n","        dim_drug_table_schema = StructType([\n","            StructField(\"Brnd_Name\", StringType(), True),\n","            StructField(\"Gnrc_Name\", StringType(), True),\n","            StructField(\"Max_Year\", IntegerType(), True),\n","            StructField(\"Min_Year\", IntegerType(), True),\n","            StructField(\"drug_key\", IntegerType(), True)\n","        ])\n","\n","        # Create empty DataFrame\n","        dim_drug_table_df = spark.createDataFrame([], dim_drug_table_schema)\n","        \n","        dim_drug_table_df \\\n","            .write.format('delta') \\\n","            .option('path',f'{lakehouse_table_dir_full_path}/{dim_drug_table_name}') \\\n","            .save()\n","\n","    if not lakehouse_table_exists(workspace_id, lakehouse_id, dim_geography_table_name):\n","        print(f'Creating {dim_geography_table_name}....')\n","        \n","        # Define schema\n","        dim_geography_table_schema = StructType([\n","            StructField(\"Prscrbr_City\", StringType(), True),\n","            StructField(\"Prscrbr_City_State\", StringType(), True),\n","            StructField(\"Prscrbr_State_Abrvtn\", StringType(), True),\n","            StructField(\"Prscrbr_State_FIPS\", StringType(), True),\n","            StructField(\"Max_Year\", IntegerType(), True),\n","            StructField(\"Min_Year\", IntegerType(), True),\n","            StructField(\"geo_key\", IntegerType(), True)\n","        ])\n","\n","        # Create empty DataFrame\n","        dim_geography_table_df = spark.createDataFrame([], dim_geography_table_schema)\n","        \n","        dim_geography_table_df \\\n","            .write.format('delta') \\\n","            .option('path',f'{lakehouse_table_dir_full_path}/{dim_geography_table_name}') \\\n","            .save()\n","\n","\n","    if not lakehouse_table_exists(workspace_id, lakehouse_id,dim_provider_table_name):\n","        print(f'Creating {dim_provider_table_name} .....')\n","\n","        # Define schema\n","        dim_provider_table_schema = StructType([\n","            StructField(\"Prscrbr_First_Name\", StringType(), True),\n","            StructField(\"Prscrbr_Full_Name\", StringType(), True),\n","            StructField(\"Prscrbr_Last_Org_Name\", StringType(), True),\n","            StructField(\"Prscrbr_NPI\", StringType(), True),\n","            StructField(\"Prscrbr_Type\", StringType(), True),\n","            StructField(\"Prscrbr_Type_Src\", StringType(), True),\n","            StructField(\"Max_Year\", IntegerType(), True),\n","            StructField(\"Min_Year\", IntegerType(), True),\n","            StructField(\"provider_key\", IntegerType(), True)\n","        ])\n","\n","        # Create empty DataFrame\n","        dim_provider_table_df = spark.createDataFrame([], dim_provider_table_schema)\n","\n","        dim_provider_table_df \\\n","            .write.format('delta') \\\n","            .option('path',f'{lakehouse_table_dir_full_path}/{dim_provider_table_name}') \\\n","            .save()\n","\n","\n","    if not lakehouse_table_exists(workspace_id, lakehouse_id, dim_year_table_name):\n","        print(f'Creating {dim_year_table_name} .....')\n","\n","        # Define schema\n","        dim_year_table_schema = StructType([\n","            StructField(\"Year\", IntegerType(), True),\n","            StructField(\"Year_Date_Key\", DateType(), True)\n","            ])\n","\n","        # Create empty DataFrame\n","        dim_year_table_df = spark.createDataFrame([], dim_year_table_schema)\n","\n","        dim_year_table_df \\\n","            .write.format('delta') \\\n","            .option('path',f'{lakehouse_table_dir_full_path}/{dim_year_table_name}') \\\n","            .save()\n","\n","\n","    if not lakehouse_table_exists(workspace_id, lakehouse_id, fact_costs_table_name):\n","        print(f'Creating {fact_costs_table_name} .....')\n","\n","        # Define schema\n","        fact_costs_table_schema = StructType([\n","            StructField(\"GE65_Bene_Sprsn_Flag\", StringType(), True),\n","            StructField(\"GE65_Sprsn_Flag\", StringType(), True),\n","            StructField(\"GE65_Tot_30day_Fills\", DecimalType(10,2), True),\n","            StructField(\"GE65_Tot_Benes\", LongType(), True),\n","            StructField(\"GE65_Tot_Clms\", LongType(), True),\n","            StructField(\"GE65_Tot_Day_Suply\", LongType(), True),\n","            StructField(\"GE65_Tot_Drug_Cst\", DecimalType(10,2), True),\n","            StructField(\"Tot_30day_Fills\", DecimalType(10,2), True),\n","            StructField(\"Tot_Benes\", LongType(), True),\n","            StructField(\"Tot_Clms\", LongType(), True),\n","            StructField(\"Tot_Day_Suply\", LongType(), True),\n","            StructField(\"Tot_Drug_Cst\", DecimalType(10,2), True),\n","            StructField(\"Year\", IntegerType(), True),\n","            StructField(\"drug_key\", IntegerType(), True),\n","            StructField(\"geo_key\", IntegerType(), True),\n","            StructField(\"provider_key\", IntegerType(), True)\n","        ])\n","\n","        # Create empty DataFrame\n","        fact_costs_table_df = spark.createDataFrame([], fact_costs_table_schema)\n","\n","        fact_costs_table_df \\\n","            .write.format('delta') \\\n","            .option('path',f'{lakehouse_table_dir_full_path}/{fact_costs_table_name}') \\\n","            .save()\n","\n","    ###########################  \n","    # Step 2: Fabric Data Agent SDK functions most likely use Lakehouse SQL Endpoint so execute metadata refresh operation using REST API\n","    #         otherwise newly created tables might not be available be set as source for Data Agent\n","\n","    #Instantiate the client\n","    client = semfabric.FabricRestClient()\n","\n","    # Get the SQL endpoint to sync with the lakehouse\n","    sqlendpoint = client.get(f\"/v1/workspaces/{workspace_id}/lakehouses/{lakehouse_id}\").json()['properties']['sqlEndpointProperties']['id']\n","\n","    # URI for the call \n","    uri = f\"v1/workspaces/{workspace_id}/sqlEndpoints/{sqlendpoint}/refreshMetadata\" \n","\n","    # payload = {} # empty payload, all tables\n","    # Example of setting a timeout, { \"timeout\": {\"timeUnit\": \"Seconds\", \"value\": \"120\"}  }\n","    payload = {} \n","\n","    fabriclient_post(uri, payload)\n","\n","    #response = client.post(uri, json= payload, lro_wait = True) \n","    #response_text = json.loads(response.text)\n","    #print(response.status_code)\n","    #print(response.text)\n","    #response.raise_for_status()  # Raise an error for bad status codes   \n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":18,"statement_ids":[18],"state":"finished","livy_statement_state":"available","session_id":"6fb88cc4-f296-4bd8-85e8-471919a7abd1","normalized_state":"finished","queued_time":"2025-10-14T16:17:01.0875423Z","session_start_time":null,"execution_start_time":"2025-10-14T16:18:24.0936014Z","execution_finish_time":"2025-10-14T16:18:56.124482Z","parent_msg_id":"7af85c89-6bd8-4a61-a42a-2aee253934bf"},"text/plain":"StatementMeta(, 6fb88cc4-f296-4bd8-85e8-471919a7abd1, 18, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/nfs4/pyenv-14667e78-08cd-419a-8571-e231b0576cb9/lib/python3.11/site-packages/sempy/fabric/_client/_rest_client.py:390: FutureWarning: The 'token_provider' parameter is deprecated and will be removed in a future version. Please use 'credential' parameter with `azure.core.credentials.TokenCredential` implementations instead.\n  super().__init__(token_provider=token_provider, retry_config=retry_config, credential=credential)\n"]},{"output_type":"stream","name":"stderr","text":["/nfs4/pyenv-14667e78-08cd-419a-8571-e231b0576cb9/lib/python3.11/site-packages/sempy/fabric/_client/_rest_client.py:390: FutureWarning: The 'token_provider' parameter is deprecated and will be removed in a future version. Please use 'credential' parameter with `azure.core.credentials.TokenCredential` implementations instead.\n  super().__init__(token_provider=token_provider, retry_config=retry_config, credential=credential)\n"]},{"output_type":"stream","name":"stderr","text":["/nfs4/pyenv-14667e78-08cd-419a-8571-e231b0576cb9/lib/python3.11/site-packages/sempy/fabric/_client/_rest_client.py:390: FutureWarning: The 'token_provider' parameter is deprecated and will be removed in a future version. Please use 'credential' parameter with `azure.core.credentials.TokenCredential` implementations instead.\n  super().__init__(token_provider=token_provider, retry_config=retry_config, credential=credential)\n"]},{"output_type":"stream","name":"stderr","text":["/nfs4/pyenv-14667e78-08cd-419a-8571-e231b0576cb9/lib/python3.11/site-packages/sempy/fabric/_client/_rest_client.py:390: FutureWarning: The 'token_provider' parameter is deprecated and will be removed in a future version. Please use 'credential' parameter with `azure.core.credentials.TokenCredential` implementations instead.\n  super().__init__(token_provider=token_provider, retry_config=retry_config, credential=credential)\n"]},{"output_type":"stream","name":"stderr","text":["/nfs4/pyenv-14667e78-08cd-419a-8571-e231b0576cb9/lib/python3.11/site-packages/sempy/fabric/_client/_rest_client.py:390: FutureWarning: The 'token_provider' parameter is deprecated and will be removed in a future version. Please use 'credential' parameter with `azure.core.credentials.TokenCredential` implementations instead.\n  super().__init__(token_provider=token_provider, retry_config=retry_config, credential=credential)\n"]},{"output_type":"stream","name":"stdout","text":["200\n{\"value\":[{\"tableName\":\"cms_provider_dim_drug\",\"status\":\"Success\",\"startDateTime\":\"2025-10-14T16:18:44.1671158Z\",\"endDateTime\":\"2025-10-14T16:18:50.5734562Z\",\"lastSuccessfulSyncDateTime\":\"2025-10-14T16:18:50.5734562Z\"},{\"tableName\":\"cms_provider_dim_year\",\"status\":\"Success\",\"startDateTime\":\"2025-10-14T16:18:44.1671158Z\",\"endDateTime\":\"2025-10-14T16:18:50.292194Z\",\"lastSuccessfulSyncDateTime\":\"2025-10-14T16:18:50.292194Z\"},{\"tableName\":\"cms_provider_dim_provider\",\"status\":\"Success\",\"startDateTime\":\"2025-10-14T16:18:44.1671158Z\",\"endDateTime\":\"2025-10-14T16:18:51.1203399Z\",\"lastSuccessfulSyncDateTime\":\"2025-10-14T16:18:51.1203399Z\"},{\"tableName\":\"cms_provider_dim_geography\",\"status\":\"Success\",\"startDateTime\":\"2025-10-14T16:18:44.1671158Z\",\"endDateTime\":\"2025-10-14T16:18:51.323421Z\",\"lastSuccessfulSyncDateTime\":\"2025-10-14T16:18:51.323421Z\"},{\"tableName\":\"cms_provider_drug_costs_star\",\"status\":\"Success\",\"startDateTime\":\"2025-10-14T16:18:44.1671158Z\",\"endDateTime\":\"2025-10-14T16:18:51.6203333Z\",\"lastSuccessfulSyncDateTime\":\"2025-10-14T16:18:51.6203333Z\"},{\"tableName\":\"cms_provider_drug_costs\",\"status\":\"Success\",\"startDateTime\":\"2025-10-14T16:18:44.1671158Z\",\"endDateTime\":\"2025-10-14T16:18:50.9327972Z\",\"lastSuccessfulSyncDateTime\":\"2025-10-14T16:18:50.9327972Z\"}]}\n"]}],"execution_count":11,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6ae7457d-1f1c-4fcf-a779-c6264cb23738"},{"cell_type":"markdown","source":["## Trigger Data Factory Pipeline to Load Data\n","\n","This step initiates the execution of the **Data Factory pipeline** to ingest and process CMS data into the Lakehouse.\n","\n","### ğŸ§  What Happens Next:\n","The pipeline orchestrates the execution of the three imported notebooks:\n","- Download CMS data\n","- Create flat data table\n","- Create star schema tables\n","\n","Once the pipeline completes, the data is available in the Lakehouse for querying and reporting.\n","\n","> â±ï¸ **Note**: The pipeline runs **asynchronously**â€”this cell only triggers the job via an API call. The notebook session does not need to remain active, and can be safely stopped after submission.\n","\n","> ğŸ“ **Monitoring Tip**: You can monitor the pipeline run from the **Monitoring Hub** or by opening the pipeline and selecting **Run > View Run History** to track progress and completion.\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8a4e837f-ae2e-4240-92ee-93e78e8da166"},{"cell_type":"code","source":["#invoke Data Factory Pipeline to load data to Lakehouse using Fabric REST API - https://learn.microsoft.com/en-us/rest/api/fabric/core/job-scheduler/run-on-demand-item-job?tabs=HTTP\n","\n","if not invoke_datafactory_pipeline_step:\n","    print('Skipping invocation of Data Factory Pipeline setp')\n","else:\n","    datafactory_pipeline_id = semfabric.resolve_item_id(datafactory_pipeline_name, type = \"DataPipeline\")\n","    print(datafactory_pipeline_id)\n","\n","    url = f\"v1/workspaces/{workspace_id}/items/{datafactory_pipeline_id}/jobs/instances?jobType=Pipeline\"\n","\n","    client = semfabric.FabricRestClient()\n","    response = client.request(method = \"POST\", path_or_url=url)\n","    print(response.status_code)\n","    print(response.text)\n","    response.raise_for_status()  # Raise an error for bad status codes   \n","\n","    print(\"Data Factory Pipeline Job submitted successfully - monitor Pipeline Run from Monitoring Hub or open the pipeline then use Run > View Run History menu to actively monitor the pipeline. Once pipeline job complete data is available in Lakehouse for querying and reporting\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":19,"statement_ids":[19],"state":"finished","livy_statement_state":"available","session_id":"6fb88cc4-f296-4bd8-85e8-471919a7abd1","normalized_state":"finished","queued_time":"2025-10-14T16:17:01.0897216Z","session_start_time":null,"execution_start_time":"2025-10-14T16:18:56.1266108Z","execution_finish_time":"2025-10-14T16:18:57.1242089Z","parent_msg_id":"cab10513-c632-4c48-ba78-58cc8ef33611"},"text/plain":"StatementMeta(, 6fb88cc4-f296-4bd8-85e8-471919a7abd1, 19, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["ea293c18-243a-43c9-9a86-f1f2dfbe20e9\n202\n\nData Factory Pipeline Job submitted successfully - monitor Pipeline Run from Monitoring Hub or open the pipeline then use Run > View Run History menu to actively monitor the pipeline. Once pipeline job complete data is available in Lakehouse for querying and reporting\n"]}],"execution_count":12,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":true,"run_control":{"frozen":false}},"id":"d5883c28-002a-4c30-8054-885fb7b53e1e"},{"cell_type":"markdown","source":["## Create Data Agent"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a8f53890-50ea-49b5-ae18-00fa4de5c2da"},{"cell_type":"code","source":["#Create Data Agent as the pre-requisite of empty table creation and SQL Endpoint metadata synch is complete\n","\n","if not invoke_data_agent_create_step:\n","\n","    print(\"Skipping Data Agent Pre-Requisite Step \")\n","\n","elif item_exists(data_agent_name, \"DataAgent\"):\n","\n","    print(f'Data Agent {data_agent_name} alerady exists so skipping the step')\n","    \n","else:    \n","    data_agent = create_data_agent(data_agent_name)\n","\n","    data_agent_instructions_local_path = base_dir_local_path + data_agent_instructions_path\n","    data_agent_instructions =  get_file_contents(data_agent_instructions_local_path)\n","    data_agent.update_configuration(instructions = data_agent_instructions)\n","\n","    data_agent.add_datasource(artifact_name_or_id = lakehouse_name, workspace_id_or_name = workspace_id, type=\"lakehouse\")\n","    datasource = data_agent.get_datasources()[0]\n","    datasource.pretty_print()\n","\n","    datasource.select(\"dbo\", dim_drug_table_name)\n","    datasource.select(\"dbo\", dim_geography_table_name)\n","    datasource.select(\"dbo\", dim_provider_table_name)\n","    datasource.select(\"dbo\", dim_year_table_name)\n","    datasource.select(\"dbo\", fact_costs_table_name)\n","\n","    data_agent_datasource_instructions_local_path = base_dir_local_path + data_agent_datasource_instructions_path\n","    data_agent_datasource_instructions = get_file_contents(data_agent_datasource_instructions_local_path)    \n","    datasource.update_configuration(instructions = data_agent_datasource_instructions)\n","\n","    data_agent_datasource_query_examples_local_path = base_dir_local_path + data_agent_datasource_query_examples_path\n","    data_agent_datasource_query_examples = get_file_contents(data_agent_datasource_query_examples_local_path)\n","    data_agent_datasource_query_examples_dict = json.loads(data_agent_datasource_query_examples)\n","    datasource.add_fewshots(data_agent_datasource_query_examples_dict)\n","\n","    data_agent.publish()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":20,"statement_ids":[20],"state":"finished","livy_statement_state":"available","session_id":"6fb88cc4-f296-4bd8-85e8-471919a7abd1","normalized_state":"finished","queued_time":"2025-10-14T16:17:01.0920779Z","session_start_time":null,"execution_start_time":"2025-10-14T16:18:57.126385Z","execution_finish_time":"2025-10-14T16:19:02.0067441Z","parent_msg_id":"d3b135c6-90ef-468c-a723-acc42d5269b4"},"text/plain":"StatementMeta(, 6fb88cc4-f296-4bd8-85e8-471919a7abd1, 20, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["cms_data_agent2 of type DataAgent does not exist\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/synfs/notebook/6fb88cc4-f296-4bd8-85e8-471919a7abd1/mnt/lakehouse/cms_lakehouse/Files/cmsdemofiles/agent_instructions.txt'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[40], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m data_agent \u001b[38;5;241m=\u001b[39m create_data_agent(data_agent_name)\n\u001b[1;32m     14\u001b[0m data_agent_instructions_local_path \u001b[38;5;241m=\u001b[39m base_dir_local_path \u001b[38;5;241m+\u001b[39m data_agent_instructions_path\n\u001b[0;32m---> 15\u001b[0m data_agent_instructions \u001b[38;5;241m=\u001b[39m  get_file_contents(data_agent_instructions_local_path)\n\u001b[1;32m     16\u001b[0m data_agent\u001b[38;5;241m.\u001b[39mupdate_configuration(instructions \u001b[38;5;241m=\u001b[39m data_agent_instructions)\n\u001b[1;32m     18\u001b[0m data_agent\u001b[38;5;241m.\u001b[39madd_datasource(artifact_name_or_id \u001b[38;5;241m=\u001b[39m lakehouse_name, workspace_id_or_name \u001b[38;5;241m=\u001b[39m workspace_id, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlakehouse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[16], line 6\u001b[0m, in \u001b[0;36mget_file_contents\u001b[0;34m(local_file_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_file_contents\u001b[39m(local_file_path):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(local_file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      7\u001b[0m         file_content \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file_content\n","File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/synfs/notebook/6fb88cc4-f296-4bd8-85e8-471919a7abd1/mnt/lakehouse/cms_lakehouse/Files/cmsdemofiles/agent_instructions.txt'"]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f5032fe9-3b66-4492-8656-1c771ffc03f6"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"widgets":{},"synapse_widget":{"version":"0.1","state":{"918ef05b-023b-4f92-8a8a-6ae76fc385e6":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"/synfs/notebook/6fb88cc4-f296-4bd8-85e8-471919a7abd1/mnt/lakehouse/cms_lakehouse/Files/cmsdemofiles","1":"/mnt/lakehouse/cms_lakehouse/Files/cmsdemofiles","2":"job","3":"abfss://c40ce2b4-2211-4459-9645-909afd3b7084@onelake.dfs.fabric.microsoft.com/ebb9147f-0489-4a34-8db2-2f9c8891426d/Files/cmsdemofiles","4":"Lakehouse"}],"schema":[{"key":"0","name":"localPath","type":"string"},{"key":"1","name":"mountPoint","type":"string"},{"key":"2","name":"scope","type":"string"},{"key":"3","name":"source","type":"string"},{"key":"4","name":"storageType","type":"string"}],"truncated":false},"isSummary":false,"language":"scala","is_jupyter":false},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":[],"seriesFieldKeys":[],"aggregationType":"sum","isStacked":false,"binsNumber":10,"wordFrequency":"-1"},"viewOptionsGroup":[{"tabItems":[{"type":"table","name":"Table","key":"0","options":{}}]}]}}}}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}