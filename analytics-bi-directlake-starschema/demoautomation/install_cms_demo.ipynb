{"cells":[{"cell_type":"code","source":["%pip install semantic-link==0.11.0 semantic-link-labs==0.11.1"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c587f17c-8494-4426-8ff4-9fb053425666"},{"cell_type":"code","source":["import requests\n","import zipfile\n","import os\n","import sempy_labs as labs\n","import sempy.fabric as semfabric\n","import base64\n","import json\n","\n","#initialization of variables\n","\n","#lakehouse\n","lakehouse_name = \"cms_lakehouse\"\n","\n","#directories - base directory is created in the lakehouse as part of the demo setup\n","base_dir_relative_path = \"Files/cmsdemofiles\"\n","\n","##enable/disable specific steps\n","skip_lakehouse_creation = True\n","skip_artificate_download_step = True\n","skip_import_notebook_step = True\n","skip_import_datafactory_pipeline_step = True\n","skip_import_semantic_model_step = True\n","skip_import_report_step = False\n","skip_invoke_datafactory_pipeline_step = True\n","\n","#external links\n","artifactzip_github_url = \"https://github.com/isinghrana/fabric-samples-healthcare/raw/refs/heads/isr-auto1/analytics-bi-directlake-starschema/demoautomation/artifacts.zip\"\n","\n","#data factory definition files are extracted from artifact zip file and paths are relative to the base dir\n","datafactory_pipeline_jsonfile_relativepath = \"/cms_pipeline.DataPipeline/pipeline-content.json\"\n","datafactory_platform_file_relativepath = \"/cms_pipeline.DataPipeline/.platform\"\n","\n","#semantic model definition files are extracted from artifact zip file and paths are relative to the base dir\n","semanticmodel_relative_path = \"/CMS_Direct_Lake_Star_Schema.SemanticModel\"\n","report_relative_path = \"/CMS Medicare Part D Star Schema.Report\"\n","\n","download_cmsdata_notebook_github_url = \"https://raw.githubusercontent.com/isinghrana/fabric-samples-healthcare/refs/heads/isr-auto1/analytics-bi-directlake-starschema/demoautomation/01-DownloadCMSDataCsvFiles.ipynb\"\n","download_cmsdata_notebook_name = \"01-DownloadCMSDataCsvFiles\"\n","\n","create_data_table_notebook_github_url = \"https://raw.githubusercontent.com/isinghrana/fabric-samples-healthcare/refs/heads/isr-auto1/analytics-bi-directlake-starschema/demoautomation/02-CreateCMSDataTable.ipynb\"\n","create_data_table_notebook_name = \"02-CreateCMSDataTable\"\n","\n","create_starschema_table_notebook_github_url = \"https://raw.githubusercontent.com/isinghrana/fabric-samples-healthcare/refs/heads/isr-auto1/analytics-bi-directlake-starschema/demoautomation/03-CreateCMSStarSchemaTables.ipynb\"\n","create_starschema_table_notebook_name = \"03-CreateCMSStarSchemaTables\"\n","\n","\n","#pipeline json has original workspace Id and noteobook ids which need to be replaced\n","replace_pipeline_workspace_id = \"904f9388-f876-4176-be2a-6ef7d62d6544\"\n","replace_pipeline_download_cmsdata_notebook_id = \"d48a1f2f-8392-4fba-bd05-f76f4b978bfe\"\n","replace_pipeline_create_data_table_notebook_id = \"b052405e-a729-4316-97bf-bbb339f86985\"\n","replace_pipeline_create_starschema_notebook_id = \"63eb8687-b7ae-4d84-9c5c-2d96b82ce74b\"\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4e1c2251-12b8-4150-8bf7-50f17ac77860"},{"cell_type":"code","source":["if (skip_lakehouse_creation):\n","    lakehouse_id = notebookutils.lakehouse.get(lakehouse_name)['id']\n","else:\n","    lakehouse = notebookutils.lakehouse.create(lakehouse_name)    \n","    lakehouse_id = lakehouse['id']\n","    \n","workspace_id = notebookutils.runtime.context[\"currentWorkspaceId\"]                                  \n","\n","#directory initialization\n","base_dir_full_path = f\"abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/{lakehouse_id}/{base_dir_relative_path}\"\n","notebookutils.fs.mkdirs(base_dir_full_path)\n","    \n","mount_point = \"/mnt/lakehouse/\" + lakehouse_name + \"/\" + base_dir_relative_path\n","print(f'base_dir full: {base_dir_full_path}, mount_point: {mount_point}')\n","\n","notebookutils.fs.mount(base_dir_full_path, mount_point)\n","base_dir_local_path = notebookutils.fs.getMountPath(mount_point)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"f526d945-1c5a-4154-9223-68dc19035f58"},{"cell_type":"code","source":["#common utility functions\n","\n","def get_file_contents(local_file_path):\n","    with open(local_file_path, \"r\", encoding=\"utf-8\") as file:\n","        file_content = file.read()\n","    return file_content\n","\n","#function is used in steps to import semantic model and report\n","#input arugment is folder with definition files\n","#directory and subdirectories are walked through and a dictionary returned where key is the part path and value is the content of the file\n","def get_fabricitemdef_partdict(definitionfiles_local_path) -> dict[str,str]:\n","\n","    def_dict = {}\n","\n","    for root, dirs, files in os.walk(definitionfiles_local_path):\n","        #print(f'Current directory: {root}')\n","        for file in files:\n","            #print(f'  File: {file}')\n","            part_key = root.replace(definitionfiles_local_path, \"\") + \"/\" + file\n","            part_key = part_key.lstrip('/')\n","            #print(f'part_key: {part_key}')\n","\n","            with open( root + \"/\" + file, \"r\", encoding=\"utf-8\") as file:\n","                payload = file.read()\n","                def_dict[part_key] = payload\n","\n","    return def_dict    \n","\n","def fabriclient_post(url, request_body):\n","\n","    client = semfabric.FabricRestClient()\n","    #print(create_datafactory_pipeline_request_body)\n","    response = client.request(method = \"POST\", path_or_url=url, lro_wait=True, json = request_body)\n","    print(response.status_code)\n","    print(response.text)\n","    response.raise_for_status()  # Raise an error for bad status codes   \n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e777955d-a2a5-4f78-8eff-8eef61a8803d"},{"cell_type":"code","source":["#download artifacts zip file - Data Factory Pipeline, Semantic Model and REport files from GitHub which be used to create corresponding Fabric Items\n","\n","def download_binary_file(url, output_path):\n","    try:        \n","        response = requests.get(url=url, stream = True)\n","        \n","        response.raise_for_status()  # Raise an error for bad status codes\n","        with open(output_path, 'wb') as file:\n","            for chunk in response.iter_content(chunk_size=8192):\n","                file.write(chunk)\n","        print(f\"File downloaded successfully to: {output_path}\")\n","    except requests.exceptions.RequestException as e:\n","        print(f\"Download failed: {e}\")\n","        raise RuntimeError(f\"Failed to download file from {url}\") from e\n","\n","\n","def unzip_file(zip_path, extract_to):\n","    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","        zip_ref.extractall(extract_to)\n","    print(f\"Extracted all contents to '{extract_to}'\")\n","\n","\n","if (skip_artificate_download_step):\n","    print (\"skipping artifact zip file download step\")    \n","else:\n","    download_path = base_dir_local_path + '/artifacts.zip'\n","    download_binary_file(artifactzip_github_url, download_path)\n","    unzip_file(download_path, base_dir_local_path)\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"389afb67-f799-497b-b550-b342000462d1"},{"cell_type":"code","source":["#import notebooks\n","\n","\n","#import the notebooks from GitHub\n","if skip_import_notebook_step:\n","    print('skipping Notebook import step')\n","else:\n","    result = labs.import_notebook_from_web(notebook_name = download_cmsdata_notebook_name, url = download_cmsdata_notebook_github_url, overwrite=True)\n","    print(f\"Import {download_cmsdata_notebook_name} result: {result}\")\n","    result = labs.import_notebook_from_web(notebook_name = create_data_table_notebook_name, url = create_data_table_notebook_github_url, overwrite=True)\n","    print(f\"Import {create_data_table_notebook_name} result: {result}\")\n","    result = labs.import_notebook_from_web(notebook_name = create_starschema_table_notebook_name, url = create_starschema_table_notebook_github_url, overwrite=True)\n","    print(f\"Import {create_starschema_table_notebook_name} result: {result}\")\n","\n","#get NotebookIds for all 3 notebooks to be used in subsequent steps\n","download_cmsdata_notebook_id = semfabric.resolve_item_id(item_name = download_cmsdata_notebook_name, type = \"Notebook\")\n","print(f\"download_cmsdata_notebook_id: {download_cmsdata_notebook_id}\")\n","\n","create_data_table_notebook_id = semfabric.resolve_item_id(item_name = create_data_table_notebook_name, type = \"Notebook\")\n","print(f\"create_data_table_notebook_id: {create_data_table_notebook_id}\")\n","\n","create_starschema_table_notebook_id = semfabric.resolve_item_id(item_name = create_starschema_table_notebook_name, type = \"Notebook\")\n","print(f\"create_starschema_notebook_id: {create_starschema_table_notebook_id}\")\n","\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"218b469a-66a6-4486-bcc9-0b3cce9f14ac"},{"cell_type":"code","source":["#import data factory pipeline\n","\n","#do no change as the data facotry pipeline name exists in the definition file as well\n","datafactory_pipeline_name = \"cms_pipeline\"\n","\n","if skip_import_datafactory_pipeline_step:\n","    print('skipping create data factory step')\n","else:\n","\n","    datafactory_pipeline_jsonfile_local_path = base_dir_local_path + datafactory_pipeline_jsonfile_relativepath\n","    datafactory_platform_file_local_path = base_dir_local_path + datafactory_platform_file_relativepath\n","\n","    #read file contents\n","    platform_file_payload =  get_file_contents(datafactory_platform_file_local_path)\n","    pipeline_json_payload =  get_file_contents(datafactory_pipeline_jsonfile_local_path)\n","\n","    #workspace id and notebook ids need to be updated/replaced from the origin pipeline definition json\n","    pipeline_json_payload = pipeline_json_payload.replace(replace_pipeline_workspace_id, workspace_id)\n","    pipeline_json_payload = pipeline_json_payload.replace(replace_pipeline_download_cmsdata_notebook_id, download_cmsdata_notebook_id)\n","    pipeline_json_payload = pipeline_json_payload.replace(replace_pipeline_create_data_table_notebook_id, create_data_table_notebook_id)\n","    pipeline_json_payload = pipeline_json_payload.replace(replace_pipeline_create_starschema_notebook_id, create_starschema_table_notebook_id)\n","\n","    #create post request body\n","    create_datafactory_pipeline_request_body = {\n","        \"displayName\": datafactory_pipeline_name,\n","        \"description\": \"cms_pipeline to ingest and process data\",\n","        \"definition\" : {\n","            \"parts\": [\n","                {\n","                    \"path\": \"pipeline-content.json\",\n","                    \"payload\": base64.b64encode(pipeline_json_payload.encode('utf-8')),\n","                    \"payloadType\": \"InlineBase64\"\n","                },\n","                {\n","                    \"path\": \".platform\",\n","                    \"payload\": base64.b64encode(platform_file_payload.encode('utf-8')),\n","                    \"payloadType\": \"InlineBase64\"\n","                }\n","            ]\n","        }\n","    }\n","\n","    create_pipeline_uri = f\"v1/workspaces/{workspace_id}/dataPipelines\"\n","\n","    client = semfabric.FabricRestClient()\n","    #print(create_datafactory_pipeline_request_body)\n","    create_datafactory_pipeline_response = client.request(method = \"POST\", path_or_url=create_pipeline_uri, lro_wait=True, json = create_datafactory_pipeline_request_body)\n","    print(create_datafactory_pipeline_response.status_code)\n","    print(create_datafactory_pipeline_response.text)\n","    create_datafactory_pipeline_response.raise_for_status()  # Raise an error for bad status codes   "],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ad8f31b3-fcba-46b3-b092-ca1a70c16523"},{"cell_type":"code","source":["#import semantic model\n","\n","#do no change as the semantic model name exists in the definition file as well\n","semantic_model_name = \"CMS_Direct_Lake_Star_Schema\" \n","\n","if (skip_import_semantic_model_step):\n","    print('skipping semantic model creation step')\n","else:    \n","    create_semantic_model_uri = f\"v1/workspaces/{workspace_id}/semanticModels\"\n","\n","    #start with body which will get populated using the model defintion \n","    create_semantic_model_request_body = {\n","        \"displayName\": semantic_model_name,\n","        \"description\": \"cms semantic model created using API\",\n","        \"definition\" : {\n","            \"parts\": []\n","            }\n","        }\n","\n","    #read the semantic model definition folder into a dictionary to be used to be populate the request body for API Post call \n","    semanticmodel_local_path = base_dir_local_path + semanticmodel_relative_path\n","    print(f'semantic model definition files path: {semanticmodel_local_path}')\n","\n","    semantic_model_part_dict = get_fabricitemdef_partdict(semanticmodel_local_path)   \n","    \n","    #populate the request body using dictionary\n","    for key, value in semantic_model_part_dict.items():        \n","        new_part = {\n","            \"path\": key,\n","            \"payload\" : base64.b64encode(value.encode('utf-8')),\n","            \"payloadType\": \"inlineBase64\"\n","        }\n","    \n","        create_semantic_model_request_body[\"definition\"][\"parts\"].append(new_part)\n","   \n","    fabriclient_post(create_semantic_model_uri, create_semantic_model_request_body)   \n","    \n","    #update the semantic model to point to lakehouse in this workspace\n","    labs.directlake.update_direct_lake_model_lakehouse_connection(\n","        dataset = semantic_model_name,\n","        lakehouse =  lakehouse_name\n","    )"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"28dfc0c0-677e-45a2-8883-bb67731af55f"},{"cell_type":"code","source":["#import report\n","\n","def update_semantic_model_id(report_def_str, id) -> str:\n","\n","    report_def_json = json.loads(report_def_str)\n","\n","    # Replace the pbiModelDatabaseName value    \n","    report_def_json[\"datasetReference\"][\"byConnection\"][\"pbiModelDatabaseName\"] = id\n","    # Convert back to JSON string\n","    updated_json_str = json.dumps(report_def_json, indent=4)\n","    #print(updated_json_str)\n","    return updated_json_str\n","\n","#do no change as the report name exists in the definition file as well\n","report_name = \"CMS Medicare Part D Star Schema\"\n","\n","if (skip_import_report_step):\n","    print('skipping report creation step')\n","else:    \n","    \n","    #need to get semantic model id because report definition.pbir file needs to be updated with the semantic model craeted as part of the setup\n","    #in this workspace\n","    semantic_model_id = semfabric.resolve_item_id(semantic_model_name, type = \"SemanticModel\")\n","    create_report_uri = f\"v1/workspaces/{workspace_id}/reports\"\n","\n","    #start with body which will get populated using the model defintion \n","    create_report_request_body = {\n","        \"displayName\": report_name,\n","        \"description\": \"report created using API\",\n","        \"definition\" : {\n","            \"parts\": []\n","            }\n","        }\n","\n","    #read the semantic model definition folder into a dictionary to be used to be populate the request body for API Post call \n","    report_local_path = base_dir_local_path + report_relative_path\n","    print(f'report definition files path: {report_local_path}')\n","\n","    report_part_dict = get_fabricitemdef_partdict(report_local_path)   \n","    \n","    #populate the request body using dictionary\n","    for key, value in report_part_dict.items():              \n","\n","        if (\"definition.pbir\" in key):\n","            value = update_semantic_model_id(value, semantic_model_id)        \n","            print(f'Updated definition json: {value}')\n","\n","        new_part = {\n","            \"path\": key,\n","            \"payload\" : base64.b64encode(value.encode('utf-8')),\n","            \"payloadType\": \"inlineBase64\"\n","        }           \n","    \n","        create_report_request_body[\"definition\"][\"parts\"].append(new_part)\n","                \n","    fabriclient_post(create_report_uri, create_report_request_body)\n","    print('report created successfully')\n","    #labs.report.report_rebind(report=report_name,dataset=semantic_model_name)  \n","   "],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9a192acb-445b-4d37-a686-ec755cbba2e6"},{"cell_type":"code","source":["#invoke Data Factory Pipeline to load data to Lakehouse\n","\n","if skip_invoke_datafactory_pipeline_step:\n","    print('Skipping invocation of Data Factory Pipeline setp')\n","else:\n","    datafactory_pipeline_id = semfabric.resolve_item_id(datafactory_pipeline_name, type = \"DataPipeline\")\n","    print(datafactory_pipeline_id)\n","\n","    url = f\"v1/workspaces/{workspace_id}/items/{datafactory_pipeline_id}/jobs/instances?jobType=Pipeline\"\n","\n","    client = semfabric.FabricRestClient()\n","    response = client.request(method = \"POST\", path_or_url=url)\n","    print(response.status_code)\n","    print(response.text)\n","    response.raise_for_status()  # Raise an error for bad status codes   \n","\n","    print(\"Data Factory Pipeline Job submitted successfully - monitor Pipeline Run from Monitoring Hub or open the pipeline then use Run > View Run History menu to actively monitor the pipeline. Once pipeline job complete data is available in Lakehouse for querying and reporting\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":true,"run_control":{"frozen":false}},"id":"d5883c28-002a-4c30-8054-885fb7b53e1e"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"version_major":2,"version_minor":0,"state":{"d7f0354919534f68bff974db8e5e2a04":{"model_name":"ProgressStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":""}},"02305d2b4b154536bda899f7d2f13617":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":" 100/100 [00:20&lt;00:00,  4.98it/s]","layout":"IPY_MODEL_5f0829eb4673481881b434ccc885e269","style":"IPY_MODEL_9ec8d6f3ee224e79a4c61f448226aa94"}},"22799791139a498c94b2e367f9598ea4":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"adc99c8f52ef440fb86e09799a85e297":{"model_name":"HBoxModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"children":["IPY_MODEL_d49ede58dd424fefb72980bb1620d01f","IPY_MODEL_6a979fe00b4b41a7ae594a993179c954","IPY_MODEL_02305d2b4b154536bda899f7d2f13617"],"layout":"IPY_MODEL_572e209e78dd4abfbe390244738cc706"}},"9ec8d6f3ee224e79a4c61f448226aa94":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"d49ede58dd424fefb72980bb1620d01f":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":"Operation v1/workspaces/1cf71c6c-daf1-4049-8485-480325b3728e/reports successfully completed: 100%","layout":"IPY_MODEL_e0254fc99a10417d9aa1bcc301b35f7a","style":"IPY_MODEL_84c4a08074f84e1584b0508cbaac718a"}},"84c4a08074f84e1584b0508cbaac718a":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"5f0829eb4673481881b434ccc885e269":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"e0254fc99a10417d9aa1bcc301b35f7a":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"572e209e78dd4abfbe390244738cc706":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"6a979fe00b4b41a7ae594a993179c954":{"model_name":"FloatProgressModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":100,"style":"IPY_MODEL_d7f0354919534f68bff974db8e5e2a04","layout":"IPY_MODEL_22799791139a498c94b2e367f9598ea4"}}}}},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}