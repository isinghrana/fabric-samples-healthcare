{"cells":[{"cell_type":"markdown","source":["### 02 - Create Lakehouse Table (Delta Parquet) from CSV files \n","This is second Notebook of the solution and uses CSV files downloaded in the previous step to create Lakehouse Table (Silver Layer) to be  used in the subsequent step for building the Gold Layer (Star Schema) tables for reporting."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e3467fa1-cc89-436f-85f2-0eded377186b"},{"cell_type":"code","source":["%run Utils"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"df8bf0b8-0f9f-47e9-b77e-cb503212a883"},{"cell_type":"code","source":["config_dict = get_config_dict()"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"661d8a83-e000-47b3-9270-6808c02d71eb"},{"cell_type":"code","source":["source_dir = \"Files/cms_raw\"\n","sourct_dir_full_abfss_path = get_full_abfss_path(config_dict['workspace_id'], config_dict['lakehouse_id'], source_dir) "],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"27777f82-7195-43ef-ae36-4ad878dbdbff"},{"cell_type":"code","source":["import os\n","from urllib.parse import urlparse\n","\n","# function to extract the name of the file form the file full path\n","# name of the file represents year value for the data for that file \n","# example file_path \"abfss://f16554c0-3959-4537-9128-ad08c6ac1692@onelake.dfs.fabric.microsoft.com/96391b77-cc32-4714-b791-70a15ccc429f/Files/cms_raw/2013.csv?version=1727404441686?flength=3375263337\"\n","# year value is 2013 which is name of the file\n","def extract_year_from_file_name(path):\n","    # Parse the URL to get the path\n","    parsed_url = urlparse(path)\n","    # Extract the file name with extension\n","    file_name_with_extension = os.path.basename(parsed_url.path)\n","    # Remove the extension\n","    file_name, extension = os.path.splitext(file_name_with_extension)\n","    return int(file_name)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"246ca1f1-b18b-4eb9-90c2-503324f6cb1d"},{"cell_type":"code","source":["from pyspark.sql.functions import udf\n","from pyspark.sql.types import IntegerType\n","\n","#create UDF wrapper around the function which extracts name of the file\n","extract_year_from_file_name_udf = udf(extract_year_from_file_name, IntegerType())"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b3d60539-fe0d-4013-8469-9795c41747a8"},{"cell_type":"code","source":["from pyspark.sql.functions import input_file_name, regexp_extract\n","\n","# load pyspark dataframe with data from all csv files \"Files/cms_raw/2013.csv\"\n","# add a column to the dataframe for the file_path using input_file_name() function \n","df = spark.read.format(\"csv\").option(\"header\",\"true\").load(sourct_dir_full_abfss_path + \"/*.csv\").withColumn(\"file_path\", input_file_name())\n","\n","# use the UDF to extract year value from the file path and add it as a column to data frame\n","df = df.withColumn(\"Year\", extract_year_from_file_name_udf(df[\"file_path\"]))\n","display(df)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"1ede7020-8f8d-4d92-902f-6506ffa668f6"},{"cell_type":"code","source":["from pyspark.sql.types import LongType, DecimalType\n","from pyspark.sql.functions import lit, col, concat\n","\n","#make minor transformation to set the appropriate data types for the various columns\n","#as well as add a few new columns based on existing columns\n","df = df.withColumn(\"Tot_Drug_Cst\", df.Tot_Drug_Cst.cast(DecimalType(10,2))) \\\n","        .withColumn(\"Tot_30day_Fills\", df.Tot_30day_Fills.cast(DecimalType(10,2))) \\\n","        .withColumn(\"GE65_Tot_30day_Fills\", df.GE65_Tot_30day_Fills.cast(DecimalType(10,2))) \\\n","        .withColumn(\"GE65_Tot_Drug_Cst\", df.GE65_Tot_Drug_Cst.cast(DecimalType(10,2))) \\\n","        .withColumn(\"Prscrbr_City_State\", concat(df.Prscrbr_City, lit(\", \"), df.Prscrbr_State_Abrvtn)) \\\n","        .withColumn(\"Prscrbr_Full_Name\", concat(df.Prscrbr_Last_Org_Name, lit(\", \"), df.Prscrbr_First_Name)) \\\n","        .withColumn(\"Tot_Clms\", df.Tot_Clms.cast(LongType())) \\\n","        .withColumn(\"Tot_Day_Suply\", df.Tot_Day_Suply.cast(LongType())) \\\n","        .withColumn(\"Tot_Benes\", df.Tot_Benes.cast(LongType())) \\\n","        .withColumn(\"GE65_Tot_Clms\", df.GE65_Tot_Clms.cast(LongType())) \\\n","        .withColumn(\"GE65_Tot_Benes\", df.GE65_Tot_Benes.cast(LongType())) \\\n","        .withColumn(\"GE65_Tot_Day_Suply\", df.GE65_Tot_Day_Suply.cast(LongType())) \\\n","        .drop(\"file_path\")\n","\n","display(df)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"63ce6084-cd92-4cb6-bdfc-1c8198adb69c"},{"cell_type":"code","source":["#write out the table to Lakehouse \n","table_name = \"cms_provider_drug_costs\"\n","dest_dir = f\"Tables/{table_name}\"\n","dest_dir_full_abfss_path = get_full_abfss_path(config_dict['workspace_id'], config_dict['lakehouse_id'], dest_dir) \n","print(dest_dir_full_abfss_path)\n","#df.write.mode(\"overwrite\").format('delta').option('path', dest_dir_full_abfss_path).saveAsTable(table_name)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"872cb42a-c209-4b9f-b292-938264d4e103"},{"cell_type":"code","source":["df.write.mode(\"overwrite\").format('delta').save(dest_dir_full_abfss_path)\n","#df.write.mode(\"overwrite\").format('delta').saveAsTable(\"cms_lakehouse2.cms_provider_drug_costs\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"abe18012-7fd5-460a-93d3-89ac79fd06f1"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}